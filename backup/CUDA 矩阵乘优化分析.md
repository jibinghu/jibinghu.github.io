#### é€šè¿‡Shared MemoryåŠ é€ŸçŸ©é˜µä¹˜(Doubleç­‰ç±»å‹)åˆ†æ
---
- [64ä½æ•°æ®çŸ©é˜µä¹˜ä¼˜åŒ–è®¿å­˜åˆ†æ](#sector_1)
- [çŸ©é˜µä¹˜æ³•çš„ CUDA ä¼˜åŒ–](#sector_2)
---

#### 64ä½æ•°æ®çŸ©é˜µä¹˜ä¼˜åŒ–è®¿å­˜åˆ†æ {#sector_1}

é€šè¿‡åˆ†æä¸‹é¢çš„ä»£ç ï¼Œå›ç­”å¯¹åº”çš„ä¸¤ä¸ªé—®é¢˜(ç­”æ¡ˆåœ¨æ–‡ç« ç»“å°¾ç»™å‡º)ã€‚

ğŸŒ°ï¼š
```cpp
#define BLOCK_SIZE 16          // å®šä¹‰å—å¤§å°ä¸º16
#define MATRIX_DIM 256         // å®šä¹‰çŸ©é˜µç»´åº¦ä¸º256

// å®šä¹‰çº¿ç¨‹å—å’Œç½‘æ ¼çš„å°ºå¯¸
dim3 Block_dim(BLOCK_SIZE, BLOCK_SIZE);
dim3 Grid_dim(MATRIX_DIM / BLOCK_SIZE, MATRIX_DIM / BLOCK_SIZE);

// å¯åŠ¨æ ¸å‡½æ•°è¿›è¡ŒçŸ©é˜µä¹˜æ³•è¿ç®—
multiply <<< Grid_dim, Block_dim >>> (A_mat_d, B_mat_d, C_mat_d);

// æ ¸å‡½æ•°ï¼Œç”¨äºæ‰§è¡ŒçŸ©é˜µä¹˜æ³•
__global__ void multiply(const double * __restrict__ A_mat, const double * __restrict__ B_mat, double * __restrict__ C_mat)
{
    int i, j;
    double temp = 0;

    // å®šä¹‰å…±äº«å†…å­˜ï¼Œç”¨äºå­˜å‚¨å—å†…çš„å­çŸ©é˜µ
    __shared__ double A_shared[BLOCK_SIZE][BLOCK_SIZE];
    __shared__ double B_shared[BLOCK_SIZE][BLOCK_SIZE];

    // è®¡ç®—å½“å‰çº¿ç¨‹å¯¹åº”çš„å…¨å±€è¡Œå’Œåˆ—ç´¢å¼•
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    // å¾ªç¯éå†æ¯ä¸ªå­çŸ©é˜µå—
    for (int tileNUM = 0; tileNUM < MATRIX_DIM / BLOCK_SIZE; tileNUM++)
    {
        // è®¡ç®—å­çŸ©é˜µå—å†…çš„ç´¢å¼•
        j = tileNUM * BLOCK_SIZE + threadIdx.x;
        i = tileNUM * BLOCK_SIZE + threadIdx.y;

        // å°†å­çŸ©é˜µå—çš„æ•°æ®åŠ è½½åˆ°å…±äº«å†…å­˜ä¸­
        A_shared[threadIdx.y][threadIdx.x] = A_mat[row * MATRIX_DIM + j];
        B_shared[threadIdx.y][threadIdx.x] = B_mat[i * MATRIX_DIM + col];
        __syncthreads();  // åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰çº¿ç¨‹éƒ½å®ŒæˆåŠ è½½

        // è¿›è¡Œå­çŸ©é˜µå—çš„ä¹˜æ³•è¿ç®—
        for (int k = 0; k < BLOCK_SIZE; k++)
        {
            temp += A_shared[threadIdx.y][k] * B_shared[k][threadIdx.x];
        }
        __syncthreads();  // å†æ¬¡åŒæ­¥ï¼Œç¡®ä¿æ‰€æœ‰çº¿ç¨‹éƒ½å®Œæˆè®¡ç®—
    }

    // å°†è®¡ç®—ç»“æœå†™å…¥å…¨å±€å†…å­˜
    C_mat[row * MATRIX_DIM + col] = temp;
}
```

æ ¹æ®ä»£ç æç¤ºåˆ†ææè¿°ï¼Œè¯·è¯´æ˜
* (1) å¯¹äºçº¿ç¨‹å—block(1,1)ï¼Œåœ¨tileNumæ˜¯1ä¸Šï¼Œä»£ç 26è¡Œæ‰§è¡Œå®Œæˆçš„æƒ…å†µä¸‹ï¼Œè¯·è¯´æ˜ï¼ŒA_sharedå’ŒB_sharedåˆ†åˆ«ä¿å­˜äº†A_matå’ŒB_matçš„å“ªäº›æ•°æ®ï¼Œä»¥åŠè¿™äº›æ•°æ®åœ¨Shared Memoryä¸­çš„å†…å­˜å¸ƒå±€ã€‚A_matå’ŒB_matä¸­çš„æ•°æ®ä»¥ç›¸å¯¹äºèµ·å§‹ä½ç½®çš„åç§»è¡¨ç¤ºï¼Œå³ç±»ä¼¼ä»£ç ä¸­A_mat[row * MATRIX_DIM + j]çš„å½¢å¼ã€‚
* (2) å¯¹äºç¬¬30è¡Œä»£ç `temp += A_shared[threadIdx.y][k] * B_shared[k][threadIdx.x];
`ï¼ŒA_sharedå’ŒB_sharedè®¿é—®shared memoryæ˜¯å¦å¯¼è‡´bank conflictsï¼Œè¯·åˆ†æè¯´æ˜ã€‚

å…³äºä¸Šè¿°é—®é¢˜çš„æ¨èé˜…è¯»ï¼š
> åœ¨åšç®—å­ä¼˜åŒ–çš„æ—¶å€™ï¼ˆæ¯”å¦‚ GEMMï¼‰ï¼Œæˆ‘ä»¬éœ€è¦å……åˆ†åˆ©ç”¨ shared memory çš„ broadcast æœºåˆ¶ï¼Œä»¥åŠé¿å… bank conflicts çš„å‡ºç°ã€‚åŒæ—¶è¿˜ä¼šç”¨ LDS.64 æˆ– LDS.128 æŒ‡ä»¤ (å±€éƒ¨æ•°æ®å…±äº«æŒ‡ä»¤/ æ±‡ç¼–çº§)ï¼ˆä¹Ÿå¯ä»¥ç›´æ¥ç”¨ float2ã€float4ï¼‰ç­‰ä¸€æ¬¡è®¿é—® 8 æˆ– 16 ä¸ª bytesã€‚ä½†åœ¨å®˜æ–¹æ–‡æ¡£ä¸­ï¼Œåªä»‹ç»äº†æ¯ä¸ª thread è®¿é—® 4 byteï¼ˆå³ 32bitï¼‰æ—¶çš„ broadcast æœºåˆ¶å’Œ bank conflict æƒ…å½¢ã€‚

å¯¹äºæ¯ä¸ªçº¿ç¨‹è®¿é—®ä¸€ä¸ªæ•°æ®çš„æƒ…å†µï¼š
* å¯¹äºå¸¸è§„è®¿å­˜æŒ‡ä»¤(32bits/4Bytes)çš„æƒ…å†µï¼Œæ¯ä¸ªwordåœ¨ i % 32 bankä¸Šï¼Œæ¯ä¸ª bank åœ¨æ¯ä¸ª cycle çš„ bandwidth ä¸º 32 bits(4 Bytes)ï¼Œæ‰€ä»¥ shared memory åœ¨æ¯ä¸ª cycle çš„ bandwidth ä¸º 32 * 32 bits = 32 * 4 bytes = 128 bytesã€‚
* å¯¹äºLSD.64(å³æ¯æ¬¡å–8Bytesæ•°æ®)çš„è®¿å­˜æŒ‡ä»¤ï¼Œ16ä¸ªçº¿ç¨‹å³å¯å–å¾—128 Bytesæ•°æ®ã€‚è¿™æ—¶CUDA ä¼šé»˜è®¤å°†ä¸€ä¸ª warp æ‹†åˆ†ä¸ºä¸¤ä¸ª half warpï¼Œæ¯ä¸ª half warp äº§ç”Ÿä¸€æ¬¡ memory transactionã€‚å³ä¸€å…±ä¸¤æ¬¡ transactionã€‚

> åªæœ‰ä»¥ä¸‹ä¸¤ä¸ªæ¡ä»¶ä¹‹ä¸€æ»¡è¶³æ—¶ï¼Œè¿™ä¸¤ä¸ª half warp çš„è®¿é—®æ‰ä¼šåˆå¹¶æˆä¸€æ¬¡ memory transactionï¼š
> * å¯¹äº Warp å†…æ‰€æœ‰æ´»è·ƒçš„ç¬¬ i å·çº¿ç¨‹ï¼Œç¬¬ i xor 1 å·çº¿ç¨‹ä¸æ´»è·ƒæˆ–è€…è®¿å­˜åœ°å€å’Œå…¶ä¸€è‡´ï¼›(i.e. T0\==T1, T2\==T3, T4\==T5, T6==T7, T8 == T9, ......, T30 == T31, etc.)
> * å¯¹äº Warp å†…æ‰€æœ‰æ´»è·ƒçš„ç¬¬ i å·çº¿ç¨‹ï¼Œç¬¬ i xor 2 å·çº¿ç¨‹ä¸æ´»è·ƒæˆ–è€…è®¿å­˜åœ°å€å’Œå…¶ä¸€è‡´ï¼›(i.e. T0\==T2, T1\==T3, T4\==T6, T5==T7 etc.)
ï¼ˆæ´»è·ƒæ˜¯æŒ‡æœ‰è®¿å­˜éœ€æ±‚ï¼‰

*ç®€å•ç†è§£ä¸€ä¸‹ï¼Œå½“ä¸Šé¢ä¸¤ç§æƒ…å†µå‘ç”Ÿæ—¶ï¼Œç¡¬ä»¶å°±å¯ä»¥åˆ¤æ–­ï¼ˆå…·ä½“æ˜¯ç¡¬ä»¶è¿˜æ˜¯ç¼–è¯‘å™¨çš„åŠŸåŠ³ï¼Œæˆ‘ä¹Ÿä¸ç¡®å®šï¼Œå…ˆå½’ç»™ç¡¬ä»¶å§ï¼‰ï¼Œå•ä¸ª half warp å†…ï¼Œæœ€å¤šéœ€è¦ 64 bytes çš„æ•°æ®ï¼Œé‚£ä¹ˆä¸¤ä¸ª half warp å°±å¯ä»¥åˆå¹¶èµ·æ¥ï¼Œé€šè¿‡ä¸€æ¬¡ memory transactionï¼Œæ‹¿å› 128 bytes çš„æ•°æ®ã€‚ç„¶åçº¿ç¨‹ä¹‹é—´æ€ä¹ˆåˆ†éƒ½å¯ä»¥ï¼ˆbroadcast æœºåˆ¶ï¼‰. å½“ç„¶ï¼Œè¿™é‡Œçš„å‰ææ˜¯æ²¡æœ‰äº§ç”Ÿ bank conflictã€‚å³æ²¡æœ‰ä»å•ä¸ª bank è¯·æ±‚è¶…è¿‡ 1 ä¸ª wordã€‚*

<div style="text-align: center;">
    <img src="https://pic2.zhimg.com/80/v2-ad56619bdac19b3a749aecd50b6b5985_1440w.webp" width="700" height="200">
    <p>æƒ…å½¢ä¸€</p>
</div>

> å…¶å® bank conflict æ˜¯é’ˆå¯¹å•æ¬¡ memory transaction è€Œè¨€çš„ã€‚å¦‚æœå•æ¬¡ memory transaction éœ€è¦è®¿é—®çš„ 128 bytes ä¸­æœ‰å¤šä¸ª word å±äºåŒä¸€ä¸ª bankï¼Œå°±äº§ç”Ÿäº† bank conflictï¼Œä»è€Œéœ€è¦æ‹†åˆ†ä¸ºå¤šæ¬¡ transactionã€‚æ¯”å¦‚è¿™é‡Œï¼Œç¬¬ä¸€æ¬¡è®¿é—®äº† 0 - 31 ä¸ª wordï¼Œç¬¬äºŒæ¬¡è®¿é—®äº† 32 - 63 ä¸ª wordï¼Œæ¯æ¬¡ transaction å†…éƒ¨å¹¶æ²¡æœ‰ bank conflictã€‚

è¿™é‡Œä¹Ÿå¯ä»¥æŸ¥çœ‹<a src="https://jibinghu.github.io/post/cong-ju-zhen-zhuan-zhi-kan-gong-xiang-nei-cun-%28CUDA%29.html">ä»çŸ©é˜µè½¬ç½®çœ‹å…±äº«å†…å­˜(CUDA)</a>

å¯¹äº128 ä½è®¿å­˜æƒ…å†µåŒç†ï¼š

> ä½¿ç”¨ LDS.128 æŒ‡ä»¤ï¼ˆæˆ–è€…é€šè¿‡ float4ã€uint4 ç­‰ç±»å‹ï¼‰å–æ•°æ®æ—¶ï¼Œæ¯ä¸ª thread è¯·æ±‚ 128 bitsï¼ˆå³ 16 bytesï¼‰æ•°æ®ï¼Œé‚£ä¹ˆæ¯ 8 ä¸ª thread å°±éœ€è¦è¯·æ±‚ 128 bytes çš„æ•°æ®ã€‚
æ‰€ä»¥ï¼ŒCUDA ä¼šé»˜è®¤æŠŠæ¯ä¸ª half warp è¿›ä¸€æ­¥åˆ‡åˆ†æˆä¸¤ä¸ª quarter warpï¼Œæ¯ä¸ªåŒ…å« 8 ä¸ª threadã€‚æ¯ä¸ª quarter warp äº§ç”Ÿä¸€æ¬¡ memory transactionã€‚æ‰€ä»¥æ¯ä¸ª warp æ¯æ¬¡è¯·æ±‚ï¼Œé»˜è®¤ä¼šæœ‰ 4 æ¬¡ memory transactionã€‚ï¼ˆæ²¡æœ‰ bank conflict çš„æƒ…å†µä¸‹ï¼‰ã€‚
> > **ç±»ä¼¼ 64 ä½å®½çš„æƒ…å†µï¼Œå½“æ»¡è¶³ç‰¹å®šæ¡ä»¶æ—¶ï¼Œä¸€ä¸ª half warp å†…çš„ä¸¤ä¸ª quarter warp çš„è®¿å­˜è¯·æ±‚ä¼šåˆå¹¶ä¸º 1 æ¬¡ memory transactionã€‚ä½†æ˜¯ä¸¤ä¸ª half warp ä¸ä¼šå†è¿›ä¸€æ­¥åˆå¹¶äº†ã€‚**

> å…·ä½“æ¡ä»¶å’Œ 64 ä½å®½ä¸€æ ·ï¼š
> * å¯¹äº Warp å†…æ‰€æœ‰æ´»è·ƒçš„ç¬¬ i å·çº¿ç¨‹ï¼Œç¬¬ i xor 1 å·çº¿ç¨‹ä¸æ´»è·ƒæˆ–è€…è®¿å­˜åœ°å€å’Œå…¶ä¸€è‡´ï¼›(i.e. T0\==T1, T2\==T3, T4\==T5, T6\==T7, T8 == T9, ......, T30 == T31, etc.)
> * å¯¹äº Warp å†…æ‰€æœ‰æ´»è·ƒçš„ç¬¬ i å·çº¿ç¨‹ï¼Œç¬¬ i xor 2 å·çº¿ç¨‹ä¸æ´»è·ƒæˆ–è€…è®¿å­˜åœ°å€å’Œå…¶ä¸€è‡´ï¼›(i.e. T0\==T2, T1\==T3, T4\==T6, T5\==T7 etc.)  (æ´»è·ƒæ˜¯æŒ‡æœ‰è®¿å­˜éœ€æ±‚ï¼‰

å¯¹äºLDS.64, LDS.128æ•°æ®çš„Bank Conflictæ›´è¯¦ç»†å†…å®¹è¯·è§Reference_1.

> [!NOTE]
> * LDS.64ï¼šä»£è¡¨ä»å±€éƒ¨æ•°æ®å…±äº«ä¸­è¯»å–æˆ–å†™å…¥ 64 ä½ï¼ˆ8 å­—èŠ‚ï¼‰çš„æ•°æ®ã€‚
    LDS.128ï¼šä»£è¡¨ä»å±€éƒ¨æ•°æ®å…±äº«ä¸­è¯»å–æˆ–å†™å…¥ 128 ä½ï¼ˆ16 å­—èŠ‚ï¼‰çš„æ•°æ®ã€‚
> * float2ï¼šè¡¨ç¤ºåŒ…å«ä¸¤ä¸ª 32 ä½æµ®ç‚¹æ•°çš„å‘é‡ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªäºŒç»´åæ ‡ (x, y) å¯ä»¥ç”¨ float2 æ¥è¡¨ç¤ºã€‚
float4ï¼šè¡¨ç¤ºåŒ…å«å››ä¸ª 32 ä½æµ®ç‚¹æ•°çš„å‘é‡ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå››ç»´å‘é‡ (x, y, z, w) æˆ–é¢œè‰²å€¼ (r, g, b, a) å¯ä»¥ç”¨ float4 æ¥è¡¨ç¤ºã€‚

---

#### çŸ©é˜µä¹˜æ³•çš„ CUDA ä¼˜åŒ– {#sector_2}

*ä»¥å•ç²¾åº¦ä¸ºä¾‹*


<div style="text-align: center;">
    <img src="https://raw.githubusercontent.com/chiemon/chiemon.github.io/master/img/cuda/1.png" width="550" height="200">
    <p>çŸ©é˜µä¹˜</p>
</div>

##### CPU ä¸‹çš„çŸ©é˜µä¹˜ï¼š

$A_{m \times k} \cdot B_{k \times n} = C_{m \times n}$

```cpp
void matrixMulCpu(float* fpMatrixA, float* fpMatrixB, float* fpMatrixC,
int m, int n, int k)
{
    float sum = 0.0f;
    for(int i = 0; i < m; i++)
    {
        for(int j = 0; j < n; j++)
        {
            for(int l = 0; l < k; l++)
            {
                sum += fpMatrixA[i * k + l] * fpMatrixB[l * n + j];
            }
            fpMatrixC[i * n + j] = sum;
            sum = 0.0f;
        }
    }
}
```
ä¸Šè¿° CPU å®ç°çš„ä»£ç ä¸­ä½¿ç”¨ä¸‰å±‚å¾ªç¯è¿›è¡ŒçŸ©é˜µä¹˜çš„è®¡ç®—ï¼Œå®é™…è®¡ç®—æ•°è¾¾åˆ°äº† $m \times n \times k$ï¼Œ æ—¶é—´å¤æ‚åº¦ä¸º $O(N^3)$.

##### GPU Native GEMM:

```cpp
__global__ void matrixMul(const float *A, const float *B, float *C, 
                          int M, int N, int K) {
    int tx = blockIdx.x * blockDim.x + threadIdx.x;
    int ty = blockIdx.y * blockDim.y + threadIdx.y;
    if(ty < M && tx < N) {
        float c = 0;
        for(int i = 0; i < K; ++i){
            c += A[ty * K + i] * B[i * N + tx];
        }
        C[ty * N + tx] = c;
    }
}
```

åœ¨ä¸Šè¿°GPU Nativeä»£ç ä¸­ï¼ŒçŸ©é˜µAéœ€è¦è¢«é‡å¤åŠ è½½ $k$ æ¬¡ï¼ŒçŸ©é˜µBéœ€è¦è¢«é‡å¤åŠ è½½ $m$ æ¬¡ï¼Œåœ¨ä¸è€ƒè™‘å…¨å±€å†…å­˜è®¿å­˜åˆå¹¶çš„æƒ…å†µä¸‹ï¼ŒçŸ©é˜µä¸­çš„æ¯ä¸ªå…ƒç´ éƒ½éœ€è¦è¢«é‡å¤åŠ è½½ $m \times n \times k$æ¬¡ã€‚åŠ ä¹‹å…¨å±€å†…å­˜è®¿é—®å»¶è¿Ÿä¸ºå‡ ç™¾ cyclesï¼Œå¯¼è‡´ GEMM æ€§èƒ½å¾ˆä½ï¼Œè®¡ç®—/è®¿å­˜æ¯”æå°ã€‚äºæ˜¯è€ƒè™‘åˆ©ç”¨å…±äº«å†…å­˜ä½å»¶è¿Ÿã€é«˜å¸¦å®½çš„ç‰¹ç‚¹ï¼Œå¯¹çŸ©é˜µä¹˜è¿›è¡Œåˆ†å—å¤„ç†ã€‚

##### å…±äº«å†…å­˜ä¼˜åŒ–çŸ©é˜µä¹˜

<div style="text-align: center;">
    <img src="https://raw.githubusercontent.com/chiemon/chiemon.github.io/master/img/cuda/5.png" width="550" height="500">
    <p>çŸ©é˜µåˆ†å—ä¹˜</p>
</div>

```cpp
// çŸ©é˜µåˆ†å—ä¹˜
__global__ void matrixMultiplySharedKernel(float* matrixA, float* matrixB, float* matrixC, int rowsA, int colsB, int commonDim) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float value = 0.0f;
    // çŸ©é˜µAå’ŒBçš„å…±äº«å†…å­˜å—
    __shared__ float tileA[TILE_SIZE][TILE_SIZE];
    __shared__ float tileB[TILE_SIZE][TILE_SIZE];
    // çŸ©é˜µåˆ†å—çš„æ•°é‡
    int numTiles = (commonDim + TILE_SIZE - 1) / TILE_SIZE;

    for(int t = 0; t < numTiles; t++) {
        // å°†å…¨å±€å†…å­˜ä¸­çš„çŸ©é˜µåˆ†å—è‡³å…±äº«å†…å­˜ä¸­
        if(row < rowsA && t * TILE_SIZE + threadIdx.x < commonDim)
            tileA[threadIdx.y][threadIdx.x] = matrixA[row * commonDim + t * TILE_SIZE + threadIdx.x];
        else
            tileA[threadIdx.y][threadIdx.x] = 0.0f;

        if(t * TILE_SIZE + threadIdx.y < commonDim && col < colsB)
            tileB[threadIdx.y][threadIdx.x] = matrixB[(t * TILE_SIZE + threadIdx.y) * colsB + col];
        else
            tileB[threadIdx.y][threadIdx.x] = 0.0f;

        __syncthreads();
        // æ¯ä¸ªçº¿ç¨‹è®¡ç®—blockå—ä¸­çš„ä¹˜ç§¯å¹¶ç´¯åŠ 
        for(int i = 0; i < TILE_SIZE; i++) {
            value += tileA[threadIdx.y][i] * tileB[i][threadIdx.x];
        }

        __syncthreads();
    }

    if(row < rowsA && col < colsB) {
        matrixC[row * colsB + col] = value;
    }
}
```

å¯¹çŸ©é˜µä¹˜è¿›è¡Œåˆ†å—å¤„ç†ï¼Œæ¯ä¸ªBlock è®¡ç®—ä¸€ä¸ªå­çŸ©é˜µï¼Œé€šè¿‡å¯¹å­çŸ©é˜µå‘ numTiles æ–¹å‘(çŸ©é˜µAå‘è¡Œæ–¹å‘ï¼ŒçŸ©é˜µBå‘åˆ—æ–¹å‘)è¿›è¡Œè¿­ä»£ï¼Œè®¡ç®—çŸ©é˜µCä¸­æ¯ä¸ªå…ƒç´ çš„å€¼ã€‚

##### å¯„å­˜å™¨ä¼˜åŒ–çŸ©é˜µä¹˜(æé«˜è®¡ç®—â¬†ï¸/è®¿å­˜æ¯”)

```cpp
// ä½¿ç”¨æŒ‡ä»¤çº§å¹¶è¡Œï¼ˆILPï¼‰2æ¥æé«˜æ€§èƒ½
__global__ void matrixMultiplySharedILPkernel(float* matrixA, float* matrixB,
float* matrixC, int rowsA, int colsB, int commonDim)
{
    int row = blockIdx.y * blockDim.y * 2 + threadIdx.y; // æ¯ä¸ªçº¿ç¨‹å—å¤„ç†ä¸¤ä¸ªå—
    int col = blockIdx.x * blockDim.x + threadIdx.x; // åˆ—ç´¢å¼•
    float value[2] = {0.0f}; // ç”¨äºå­˜å‚¨ä¸¤ä¸ªç»“æœå€¼

    __shared__ float tileA[BLOCK_SIZE][BLOCK_SIZE]; // å…±äº«å†…å­˜ä¸­çš„å­çŸ©é˜µA
    __shared__ float tileB[BLOCK_SIZE][BLOCK_SIZE]; // å…±äº«å†…å­˜ä¸­çš„å­çŸ©é˜µB

    int numTiles = (commonDim + BLOCK_SIZE - 1) / BLOCK_SIZE; // è®¡ç®—éœ€è¦å¤„ç†çš„å­çŸ©é˜µå—æ•°
    for(int t = 0; t < numTiles; t++)
    {
        // ä»å…¨å±€å†…å­˜åŠ è½½æ•°æ®åˆ°å…±äº«å†…å­˜
        tileA[threadIdx.y][threadIdx.x] = matrixA[row * commonDim + t * BLOCK_SIZE + threadIdx.x];
        tileA[threadIdx.y + 16][threadIdx.x] = matrixA[(row + 16) * commonDim + t * BLOCK_SIZE + threadIdx.x];

        tileB[threadIdx.y][threadIdx.x] = matrixB[(t * BLOCK_SIZE + threadIdx.y) * colsB + col];
        tileB[threadIdx.y + 16][threadIdx.x] = matrixB[(t * BLOCK_SIZE + threadIdx.y + 16) * colsB + col];

        __syncthreads(); // ç¡®ä¿æ‰€æœ‰çº¿ç¨‹éƒ½å®ŒæˆåŠ è½½

        // å­çŸ©é˜µä¹˜æ³•
        for(int j = 0; j < BLOCK_SIZE; j++)
        {
            value[0] += tileA[threadIdx.y][j] * tileB[j][threadIdx.x];
            value[1] += tileA[threadIdx.y + 16][j] * tileB[j][threadIdx.x];
        }

        __syncthreads(); // ç¡®ä¿æ‰€æœ‰çº¿ç¨‹éƒ½å®Œæˆè®¡ç®—
    }

    // å°†è®¡ç®—ç»“æœå†™å…¥å…¨å±€å†…å­˜
    matrixC[row * colsB + col] = value[0];
    matrixC[(row + 16) * colsB + col] = value[1];
}
```

> æ³¨æ„, kernel launch æ—¶çš„ blocksize éœ€è¦å˜åŒ–ä¸º: blockSize.y = BLOCK_SIZE / 2ã€‚è€Œ gridSize ä¸å˜.

äº‹å®ä¸Šï¼Œä¸Šè¿°ç¨‹åºå¯¹è®¡ç®—/è®¿å­˜æ¯”çš„æé«˜å¹¶ä¸æ˜æ˜¾ï¼Œè¦æ˜¾è‘—æé«˜ç¨‹åºè¿è¡Œæ•ˆç‡å¯ä»¥é‡‡ç”¨å‘é‡åŒ–è®¿å­˜æŒ‡ä»¤ã€‚

```cpp
float c[4][4] = {{0}};
    float a_reg[4];
    float b_reg[4];
    for(int i = 0; i < K; i += TILE_K){
        __syncthreads();
        // transfer tile from global mem to shared mem
        load_gmem_tile_to_smem(A, i, smemA);
        load_gmem_tile_to_smem(B, i, smemB);
        __syncthreads();
    #pragma unroll
        for(int j = 0; j < TILE_K; ++j) {
            // load tile from shared mem to register 
            load_smem_tile_to_reg(smemA, j, a_reg);
            load_smem_tile_to_reg(smemB, j, b_reg);
            // compute matrix multiply accumulate 4x4
            mma4x4(a_reg, b_reg, c)ï¼›
        }
    }
```

ä¸Šè¿°ä¼ªä»£ç ç¨‹åºä¸­æ¯æ¬¡è®¿å­˜å‘é‡åŒ–è¾¾åˆ°æé«˜è®¡ç®—/è®¿å­˜æ¯”çš„æ•ˆæœã€‚



<div style="text-align: center;">
    <img src="https://img2024.cnblogs.com/blog/3358182/202406/3358182-20240610204152425-44254021.png" width="550" height="500">
    <p>çŸ©é˜µåˆ†å—ä¹˜ç­–ç•¥</p>
</div>

> è€ƒè™‘ä¸€ä¸ª block è®¡ç®— 128x128 çš„åˆ†å—ï¼Œè‹¥æ¯ä¸ªçº¿ç¨‹è®¡ç®— 128 ä¸ªç»“æœï¼Œéœ€è¦çš„ block size ä¸º 128ï¼Œå•ä¸ªçº¿ç¨‹éœ€è¦ 128 ä¸ªå¯„å­˜å™¨å‚¨å­˜è®¡ç®—ç»“æœï¼ŒåŠ ä¸Šæ‰€éœ€çš„ Gmem to Smemï¼ŒSmem to Reg ç­‰ä¸€äº›æ‰€éœ€çš„å¯„å­˜å™¨ï¼Œå¤§æ¦‚å…±éœ€è¦è‡³å°‘ 180 å¤šä¸ªï¼Œè®¡ç®— Occupany å¯çŸ¥æ­¤æ—¶çš„ Active Warp æ•°åªæœ‰ 8ï¼ŒOccupany ä¸º 25%ï¼›è‹¥è®¾ç½® block size ä¸º 256ï¼Œåˆ™æ¯ä¸ªçº¿ç¨‹ä»…éœ€è®¡ç®— 64 ä¸ªç»“æœï¼Œè°ƒæ•´å¯„å­˜å™¨å’Œ Shared Memory çš„ä½¿ç”¨é‡å¹¶è§‚å¯Ÿ Occupanyï¼Œå¯çŸ¥è‹¥æ¯ä¸ªçº¿ç¨‹åªä½¿ç”¨ 128 ä¸ªå¯„å­˜å™¨ï¼Œblock å†…çš„ Shared Memory ä½¿ç”¨é‡é™åˆ¶åœ¨ 32Kï¼ŒActive Warp æ•°å¯ä»¥è¾¾åˆ° 16ï¼Œæ˜¯ä¸€ä¸ªæ›´ä¼˜çš„é€‰æ‹©ã€‚


```cpp
#define TILE_K 16
    __shared__ float4 smemA[2][TILE_K * 128 / 4];
    __shared__ float4 smemB[2][TILE_K * 128 / 4];
    float4 c[8][2] = {{make_float4(0.f, 0.f, 0.f, 0.f)}};
    float4 ldg_a_reg[2];
    float4 ldg_b_reg[2];
    float4 a_reg[2][2];
    float4 b_reg[2][2];

    // transfer first tile from global mem to shared mem
    load_gmem_tile_to_reg(A, 0, ldg_a_reg);
    load_gmem_tile_to_reg(B, 0, ldg_b_reg);

    store_reg_to_smem_tile_transpose(ldg_a_reg, 0, smemA[0]);
    store_reg_to_smem_tile(ldg_b_reg, 0, smemB[0]);
    __syncthreads();

    // load first tile from shared mem to register 
    load_smem_tile_to_reg(smemA[0], 0, a_reg[0]);
    load_smem_tile_to_reg(smemB[0], 0, b_reg[0]);

    int write_stage_idx = 1; //ping pong switch
    do {
        i += TILE_K;
        // load next tile from global mem
        load_gmem_tile_to_reg(A, i, ldg_a_reg);
        load_gmem_tile_to_reg(B, i, ldg_b_reg);

        int load_stage_idx = write_stage_idx ^ 1;

    #pragma unroll
        for(int j = 0; j < TILE_K - 1; ++j) {
            // load next tile from shared mem to register 
            load_smem_tile_to_reg(smemA[load_stage_idx], j + 1, a_reg[(j + 1) % 2]);
            load_smem_tile_to_reg(smemB[load_stage_idx], j + 1, b_reg[(j + 1) % 2]);
            // compute matrix multiply accumulate 8x8
            mma8x8(a_reg[j % 2], b_reg[j % 2], c)ï¼›
        }

        if(i < K) {
            // store next tile to shared mem
            store_reg_to_smem_tile_transpose(ldg_a_reg, 0, smemA[write_stage_idx]);
            store_reg_to_smem_tile(ldg_b_reg, 0, smemB[write_stage_idx]);
            // use double buffer, only need one sync
            __syncthreads();
            // switch
            write_stage_idx ^= 1;
        }

        // load first tile from shared mem to register of next iter
        load_smem_tile_to_reg(smemA[load_stage_idx ^ 1], 0, a_reg[0]);
        load_smem_tile_to_reg(smemB[load_stage_idx ^ 1], 0, b_reg[0]);
        // compute last tile mma 8x8
        mma8x8(a_reg[1], b_reg[1], c)ï¼›
    } while (i < K);

    store_c(c, C)
```
è¿™æ®µä¼ªä»£ç å®ç°äº†ä¸€ä¸ªçŸ©é˜µä¹˜æ³•çš„CUDAæ ¸å‡½æ•°ï¼Œå…¶ä¸­ä½¿ç”¨äº†å…±äº«å†…å­˜å’Œå¯„å­˜å™¨æ¥ä¼˜åŒ–æ•°æ®è®¿é—®é€Ÿåº¦ï¼Œé‡‡ç”¨åŒç¼“å†²æœºåˆ¶æ¥æé«˜è®¡ç®—æ•ˆç‡ã€‚ä¸‹é¢æ˜¯å¯¹è¿™æ®µä»£ç çš„é€æ­¥ä¸­æ–‡è§£æï¼š

###### å˜é‡å®šä¹‰å’Œåˆå§‹åŒ–

1. **å®å®šä¹‰**ï¼š`TILE_K` å®šä¹‰ä¸º16ï¼Œè¿™è¡¨ç¤ºæ¯ä¸ª tile çš„å¤§å°ä¸º 16ã€‚
2. **å…±äº«å†…å­˜**ï¼š
   - `smemA` å’Œ `smemB` æ˜¯ä¸¤ä¸ªäºŒç»´å…±äº«å†…å­˜æ•°ç»„ï¼Œæ¯ä¸ªæ•°ç»„æœ‰ä¸¤ä¸ªç¼“å†²åŒºï¼Œæ¯ä¸ªç¼“å†²åŒºå¤§å°ä¸º `TILE_K * 128 / 4`ã€‚
3. **å¯„å­˜å™¨**ï¼š
   - `c` æ˜¯ç”¨äºå­˜å‚¨ç»“æœçš„å¯„å­˜å™¨æ•°ç»„ï¼Œåˆå§‹åŒ–ä¸ºå…¨é›¶ã€‚
   - `ldg_a_reg` å’Œ `ldg_b_reg` æ˜¯ç”¨äºä»å…¨å±€å†…å­˜åŠ è½½æ•°æ®åˆ°å¯„å­˜å™¨çš„ä¸´æ—¶å¯„å­˜å™¨æ•°ç»„ã€‚
   - `a_reg` å’Œ `b_reg` æ˜¯ç”¨äºä»å…±äº«å†…å­˜åŠ è½½æ•°æ®åˆ°å¯„å­˜å™¨çš„ä¸´æ—¶å¯„å­˜å™¨æ•°ç»„ã€‚

###### åˆå§‹åŒ–é˜¶æ®µ

4. **ä»å…¨å±€å†…å­˜åŠ è½½ç¬¬ä¸€å—æ•°æ®åˆ°å¯„å­˜å™¨**ï¼š
   - `load_gmem_tile_to_reg` å‡½æ•°ä»å…¨å±€å†…å­˜ä¸­åŠ è½½æ•°æ®åˆ° `ldg_a_reg` å’Œ `ldg_b_reg`ã€‚
5. **å°†å¯„å­˜å™¨æ•°æ®å­˜å‚¨åˆ°å…±äº«å†…å­˜**ï¼š
   - `store_reg_to_smem_tile_transpose` å°† `ldg_a_reg` çš„æ•°æ®è½¬ç½®åå­˜å‚¨åˆ°å…±äº«å†…å­˜ `smemA[0]` ä¸­ã€‚
   - `store_reg_to_smem_tile` å°† `ldg_b_reg` çš„æ•°æ®å­˜å‚¨åˆ°å…±äº«å†…å­˜ `smemB[0]` ä¸­ã€‚
6. **åŒæ­¥çº¿ç¨‹**ï¼š
   - `__syncthreads()` ç”¨äºç¡®ä¿æ‰€æœ‰çº¿ç¨‹éƒ½å®Œæˆäº†å¯¹å…±äº«å†…å­˜çš„å†™å…¥ã€‚

7. **ä»å…±äº«å†…å­˜åŠ è½½æ•°æ®åˆ°å¯„å­˜å™¨**ï¼š
   - `load_smem_tile_to_reg` å°†å…±äº«å†…å­˜ `smemA[0]` å’Œ `smemB[0]` ä¸­çš„æ•°æ®åŠ è½½åˆ°å¯„å­˜å™¨ `a_reg[0]` å’Œ `b_reg[0]`ã€‚

###### è®¡ç®—é˜¶æ®µ

8. **åˆå§‹åŒ–åŒç¼“å†²å¼€å…³**ï¼š`write_stage_idx` è®¾ç½®ä¸º 1ã€‚

9. **ä¸»è®¡ç®—å¾ªç¯**ï¼šä½¿ç”¨ `do-while` å¾ªç¯æ¥éå†æ•´ä¸ª K ç»´åº¦ã€‚
   - **æ›´æ–°å…¨å±€å†…å­˜æŒ‡é’ˆ**ï¼š`i += TILE_K`ã€‚
   - **åŠ è½½ä¸‹ä¸€å—æ•°æ®åˆ°å¯„å­˜å™¨**ï¼šä»å…¨å±€å†…å­˜ä¸­åŠ è½½ä¸‹ä¸€å—æ•°æ®åˆ° `ldg_a_reg` å’Œ `ldg_b_reg`ã€‚

10. **è®¡ç®—å¾ªç¯**ï¼šä½¿ç”¨ `#pragma unroll` å±•å¼€å†…éƒ¨å¾ªç¯ï¼ŒåŠ é€Ÿè®¡ç®—ã€‚
    - **ä»å…±äº«å†…å­˜åŠ è½½ä¸‹ä¸€å—æ•°æ®åˆ°å¯„å­˜å™¨**ï¼šæ¯æ¬¡ä»å…±äº«å†…å­˜åŠ è½½ä¸‹ä¸€è¡Œ tile æ•°æ®åˆ°å¯„å­˜å™¨ `a_reg[(j + 1) % 2]` å’Œ `b_reg[(j + 1) % 2]`ã€‚
    - **çŸ©é˜µä¹˜æ³•ç´¯ç§¯è®¡ç®—**ï¼šè°ƒç”¨ `mma8x8` å‡½æ•°è¿›è¡Œ 8x8 çš„çŸ©é˜µä¹˜æ³•ç´¯ç§¯ã€‚

11. **å¦‚æœæ²¡æœ‰åˆ°è¾¾æœ€åä¸€å—æ•°æ®**ï¼š
    - **å°†å¯„å­˜å™¨æ•°æ®å­˜å‚¨åˆ°å…±äº«å†…å­˜**ï¼š
      - `store_reg_to_smem_tile_transpose` å°† `ldg_a_reg` çš„æ•°æ®è½¬ç½®åå­˜å‚¨åˆ°å…±äº«å†…å­˜ `smemA[write_stage_idx]` ä¸­ã€‚
      - `store_reg_to_smem_tile` å°† `ldg_b_reg` çš„æ•°æ®å­˜å‚¨åˆ°å…±äº«å†…å­˜ `smemB[write_stage_idx]` ä¸­ã€‚
    - **åŒæ­¥çº¿ç¨‹**ï¼šç¡®ä¿æ‰€æœ‰çº¿ç¨‹å®Œæˆå¯¹å…±äº«å†…å­˜çš„å†™å…¥ã€‚
    - **åˆ‡æ¢åŒç¼“å†²åŒº**ï¼š`write_stage_idx ^= 1`ã€‚

12. **åŠ è½½ä¸‹ä¸€æ¬¡è¿­ä»£çš„ç¬¬ä¸€å—æ•°æ®**ï¼š
    - **ä»å…±äº«å†…å­˜åŠ è½½æ•°æ®åˆ°å¯„å­˜å™¨**ï¼šä»å…±äº«å†…å­˜ `smemA[load_stage_idx ^ 1]` å’Œ `smemB[load_stage_idx ^ 1]` åŠ è½½æ•°æ®åˆ° `a_reg[0]` å’Œ `b_reg[0]`ã€‚

13. **è®¡ç®—æœ€åä¸€å—æ•°æ®**ï¼šè°ƒç”¨ `mma8x8` å‡½æ•°è¿›è¡Œæœ€åä¸€å—æ•°æ®çš„çŸ©é˜µä¹˜æ³•ç´¯ç§¯ã€‚

###### å­˜å‚¨ç»“æœ

14. **å°†è®¡ç®—ç»“æœå­˜å‚¨åˆ°å…¨å±€å†…å­˜**ï¼š`store_c` å‡½æ•°å°†å¯„å­˜å™¨ `c` ä¸­çš„è®¡ç®—ç»“æœå­˜å‚¨åˆ°å…¨å±€å†…å­˜ `C` ä¸­ã€‚

è¿™æ®µä»£ç é€šè¿‡ä½¿ç”¨å…±äº«å†…å­˜å’Œå¯„å­˜å™¨è¿›è¡ŒåŒç¼“å†²æ•°æ®ä¼ è¾“ï¼Œå®ç°äº†é«˜æ•ˆçš„çŸ©é˜µä¹˜æ³•è®¡ç®—ã€‚è¿™æ ·å¯ä»¥åœ¨ä¸€æ¬¡è¿­ä»£ä¸­å®Œæˆæ•°æ®çš„åŠ è½½ã€è®¡ç®—å’Œå­˜å‚¨æ“ä½œï¼Œæœ‰æ•ˆåœ°æé«˜äº†è®¡ç®—æ•ˆç‡ã€‚

---

**æ­¤å¤–ï¼Œè¿˜å¯ä»¥åˆ©ç”¨ CUDA 9 çš„warp-levelæŒ‡ä»¤ä»¥åŠTensor Coreè¿›è¡Œ GEMM çš„ä¼˜åŒ–ï¼š**

##### åˆ©ç”¨Tensor Coreè¿›è¡ŒGEMMä¼˜åŒ–

```cpp
__global__ void tensorCoreMatrixMul(float* A, float* B, float* C, int M, int N, int K) {
#if __CUDA_ARCH__ >= 700
    // é€‚ç”¨äº Tensor Cores çš„æ•°æ®ç±»å‹å¿…é¡»æ˜¯ half
    half* A_half = reinterpret_cast<half*>(A);
    half* B_half = reinterpret_cast<half*>(B);
    half* C_half = reinterpret_cast<half*>(C);

    int warpM = threadIdx.y / 8;
    int warpN = threadIdx.x / 8;
    int laneM = threadIdx.y % 8;
    int laneN = threadIdx.x % 8;

    wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
    wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::col_major> b_frag;
    wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;

    wmma::fill_fragment(c_frag, 0.0f);

    for (int i = 0; i < K; i += 16) {
        int aRow = warpM * 16;
        int bCol = warpN * 16;

        wmma::load_matrix_sync(a_frag, A_half + aRow * K + i, K);
        wmma::load_matrix_sync(b_frag, B_half + i * N + bCol, N);

        wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);
    }

    int cRow = warpM * 16;
    int cCol = warpN * 16;

    wmma::store_matrix_sync(C_half + cRow * N + cCol, c_frag, N, wmma::mem_row_major);
#endif
}
```

---

**æ–‡ç« å¼€å¤´é—®é¢˜ç­”æ¡ˆï¼š**
1. ç›´æ¥è®¡ç®—å³å¯ï¼›

2. A_sharedå’ŒB_sharedè®¿é—®shared memoryéƒ½ä¸ä¼šå¯¼è‡´bank conflictsï¼šA_sharedä¼šè§¦å‘å¹¿æ’­æœºåˆ¶ï¼Œè€ŒB_sharedåˆ™å®ç°äº†è¿ç»­è®¿é—®ã€‚
    > [!IMPORTANT]
    > æ³¨æ„è¿™é‡Œå¾ˆéš¾ç†è§£çš„ä¸€ç‚¹ï¼Œåœ¨(double)float32ç±»å‹ä¸­ï¼Œå®é™…ä¸Šæ˜¯å°†16 ä¸ªthreads ä½œä¸ºhalf warpçš„ã€‚ä¹Ÿå°±æ˜¯è¯´æ¯ä¸€è¡Œ32 ä¸ªbankå¯¹åº”çš„å°±æ˜¯16 ä¸ªçº¿ç¨‹ï¼Œæ­¤æ—¶å¯¹åº”çŸ©é˜µä¹˜æ¥è¯´ï¼Œtemp += A_shared[threadIdx.y][k] * B_shared[k][threadIdx.x]; æ¯æ¬¡k çš„è¿­ä»£ä¸­å¯¹äºA_sharedå…±äº«å†…å­˜éƒ½è¿›è¡Œäº†å¹¿æ’­ï¼Œè€Œå¯¹äºB_sharedå…±äº«å†…å­˜åˆ™æ˜¯è¿ç»­è®¿é—®ï¼Œéƒ½æ²¡æœ‰bank conflict.


**REFERENCE:THANKS FOR**

<a href="https://zhuanlan.zhihu.com/p/690052715?utm_id=0">1. ææ‡‚ CUDA Shared Memory ä¸Šçš„ bank conflicts å’Œå‘é‡åŒ–æŒ‡ä»¤ï¼ˆLDS.128 / float4ï¼‰çš„è®¿å­˜ç‰¹ç‚¹</a>

<a href="https://chiemon.github.io/2020/02/06/CUDA-%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95-%E4%BC%98%E5%8C%96%E5%8F%8A%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90-%E4%B8%8A.html">2. çŸ©é˜µä¹˜æ³•çš„ CUDA å®ç°ã€ä¼˜åŒ–åŠæ€§èƒ½åˆ†æ</a>

<a href="https://zhuanlan.zhihu.com/p/410278370">3. CUDA çŸ©é˜µä¹˜æ³•ç»ˆæä¼˜åŒ–æŒ‡å—</a>