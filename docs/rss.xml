<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>ZOMBIE_</title><link>https://jibinghu.github.io</link><description>我可能当不了绝世高手</description><copyright>ZOMBIE_</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://q6.itc.cn/q_70/images01/20240415/2cdb0abd9b724802baff3b9199d3fbc4.jpeg</url><title>avatar</title><link>https://jibinghu.github.io</link></image><lastBuildDate>Tue, 03 Sep 2024 13:55:09 +0000</lastBuildDate><managingEditor>ZOMBIE_</managingEditor><ttl>60</ttl><webMaster>ZOMBIE_</webMaster><item><title>ONNX配置参数说明（v1）</title><link>https://jibinghu.github.io/post/ONNX-pei-zhi-can-shu-shuo-ming-%EF%BC%88v1%EF%BC%89.html</link><description>ONNX配置参数说明（v1）&#13;
&#13;
torch Version 2.1.0 的 export参数：&#13;
``` python&#13;
def export(&#13;
    model: Union[torch.nn.Module, torch.jit.ScriptModule, torch.jit.ScriptFunction],&#13;
    args: Union[Tuple[Any, ...], torch.Tensor],&#13;
    f: Union[str, io.BytesIO],&#13;
    export_params: bool = True,&#13;
    verbose: bool = False,&#13;
    training: _C_onnx.TrainingMode = _C_onnx.TrainingMode.EVAL,&#13;
    input_names: Optional[Sequence[str]] = None,&#13;
    output_names: Optional[Sequence[str]] = None,&#13;
    operator_export_type: _C_onnx.OperatorExportTypes = _C_onnx.OperatorExportTypes.ONNX,&#13;
    opset_version: Optional[int] = None,&#13;
    do_constant_folding: bool = True,&#13;
    dynamic_axes: Optional[&#13;
        Union[Mapping[str, Mapping[int, str]], Mapping[str, Sequence[int]]]&#13;
    ] = None,&#13;
    keep_initializers_as_inputs: Optional[bool] = None,&#13;
    custom_opsets: Optional[Mapping[str, int]] = None,&#13;
    export_modules_as_functions: Union[bool, Collection[Type[torch.nn.Module]]] = False,&#13;
    autograd_inlining: Optional[bool] = True,&#13;
)&#13;
```&#13;
&#13;
model: pytorch模型&#13;
args: 第一个参数model的输入数据，因为模型的输入可能不止一个，因此采用元组作为参数&#13;
export_params: 导出的onnx模型文件可以包含网络结构与权重参数，如果设置该参数为False，则导出的onnx模型文件只包含网络结构，因此，一般保持默认为True即可&#13;
verbose: 该参数如果指定为True，则在导出onnx的过程中会打印详细的导出过程信息&#13;
&#13;
opset_version: ONNX的算子集版本，默认为11。</description><guid isPermaLink="true">https://jibinghu.github.io/post/ONNX-pei-zhi-can-shu-shuo-ming-%EF%BC%88v1%EF%BC%89.html</guid><pubDate>Tue, 03 Sep 2024 13:54:39 +0000</pubDate></item><item><title>CUDA 编程模型中的 Block 的共享内存与 SM 的L1 Cache和Shared Memory</title><link>https://jibinghu.github.io/post/CUDA%20-bian-cheng-mo-xing-zhong-de-%20Block%20-de-gong-xiang-nei-cun-yu-%20SM%20-de-L1%20Cache-he-Shared%20Memory.html</link><description>### CUDA 编程模型中的 Block 的共享内存与 SM 的L1 Cache和Shared Memory有什么区别和联系？&#13;
&#13;
在 CUDA 编程模型中，Block 的共享内存（Shared Memory）与 SM（Streaming Multiprocessor）的 L1 Cache 和 Shared Memory 是两个重要的内存层级，它们在用途、性能和实现上都有所不同。</description><guid isPermaLink="true">https://jibinghu.github.io/post/CUDA%20-bian-cheng-mo-xing-zhong-de-%20Block%20-de-gong-xiang-nei-cun-yu-%20SM%20-de-L1%20Cache-he-Shared%20Memory.html</guid><pubDate>Mon, 02 Sep 2024 06:06:48 +0000</pubDate></item><item><title>NV知识库( )</title><link>https://jibinghu.github.io/post/NV-zhi-shi-ku-%28%20%29.html</link><description>### SASS 和 PTX&#13;
&#13;
SASS(Streaming Assembler) 和 PTX(Parallel Thread Execution)都是 NVIDIA CUDA 编程模型中的组件，处于不同的抽象层次。</description><guid isPermaLink="true">https://jibinghu.github.io/post/NV-zhi-shi-ku-%28%20%29.html</guid><pubDate>Wed, 28 Aug 2024 10:11:47 +0000</pubDate></item><item><title> How_to_optimize_in_GPU_GEMM_(二)</title><link>https://jibinghu.github.io/post/%20How_to_optimize_in_GPU_GEMM_%28-er-%29.html</link><description>&lt;a href='https://github.com/Liu-xiandong/How_to_optimize_in_GPU'&gt; How_to_optimize_in_GPU_GEMM_(二)_评论分析&lt;/a&gt;&#13;
---&#13;
你好想问一下看起来并没有用异步的指令为什么可以实现数据预取呢&#13;
&gt; pipeline 双缓冲 pingpong操作，一个事情，都是为了实现计算和访存错开。</description><guid isPermaLink="true">https://jibinghu.github.io/post/%20How_to_optimize_in_GPU_GEMM_%28-er-%29.html</guid><pubDate>Sun, 25 Aug 2024 14:50:34 +0000</pubDate></item><item><title>海光 DCU 相关知识</title><link>https://jibinghu.github.io/post/hai-guang-%20DCU%20-xiang-guan-zhi-shi.html</link><description>&gt; 目前还是在学习阶段，把之后可能时常需要用到的技术备忘在这里。</description><guid isPermaLink="true">https://jibinghu.github.io/post/hai-guang-%20DCU%20-xiang-guan-zhi-shi.html</guid><pubDate>Fri, 23 Aug 2024 14:51:13 +0000</pubDate></item><item><title>C++模板的使用</title><link>https://jibinghu.github.io/post/C%2B%2B-mo-ban-de-shi-yong.html</link><description>模板是C++支持[参数化](https://so.csdn.net/so/search?q=%E5%8F%82%E6%95%B0%E5%8C%96&amp;spm=1001.2101.3001.7020)多态的工具，模板的参数有三种类型：类型参数、非类型参数和模板类型参数。</description><guid isPermaLink="true">https://jibinghu.github.io/post/C%2B%2B-mo-ban-de-shi-yong.html</guid><pubDate>Fri, 23 Aug 2024 07:46:25 +0000</pubDate></item><item><title>AWQ量化</title><link>https://jibinghu.github.io/post/AWQ-liang-hua.html</link><description>挑选显著权重：权重矩阵的一行作为一个单位。</description><guid isPermaLink="true">https://jibinghu.github.io/post/AWQ-liang-hua.html</guid><pubDate>Wed, 21 Aug 2024 09:51:57 +0000</pubDate></item><item><title>基座模型私有数据训练</title><link>https://jibinghu.github.io/post/ji-zuo-mo-xing-si-you-shu-ju-xun-lian.html</link><description>针对基座模型（例如大型语言模型）进行私有数据训练，以下是几种代价较小的方式：&#13;
&#13;
1. QLoRA 微调&#13;
- 概念：QLoRA（Quantized Low Rank Adaptation）是一种利用低秩矩阵分解和量化技术的微调方法，能够在模型参数显著减少的情况下，实现类似全量模型微调的效果。</description><guid isPermaLink="true">https://jibinghu.github.io/post/ji-zuo-mo-xing-si-you-shu-ju-xun-lian.html</guid><pubDate>Wed, 21 Aug 2024 09:50:57 +0000</pubDate></item><item><title>Pre-Norm&amp;Post-Norm</title><link>https://jibinghu.github.io/post/Pre-Norm%26Post-Norm.html</link><description>![](https://img2024.cnblogs.com/blog/3358182/202407/3358182-20240717110533959-1175740727.png)&#13;
&#13;
&#13;
从图中可以看出，两种不同的Transformer结构：Post-Norm Residual Unit 和 Pre-Norm Residual Unit。</description><guid isPermaLink="true">https://jibinghu.github.io/post/Pre-Norm%26Post-Norm.html</guid><pubDate>Wed, 17 Jul 2024 03:07:42 +0000</pubDate></item><item><title>CUDA binary analysis utils</title><link>https://jibinghu.github.io/post/CUDA%20binary%20analysis%20utils.html</link><description>cuobjdump ：&#13;
cuobjdump 是 NVIDIA 提供的一个工具，用于提取和显示 CUDA 二进制文件（即 CUDA 应用程序的可执行文件）中的信，可以用来分析cubin文件和host文件。</description><guid isPermaLink="true">https://jibinghu.github.io/post/CUDA%20binary%20analysis%20utils.html</guid><pubDate>Tue, 16 Jul 2024 08:21:47 +0000</pubDate></item><item><title>C++中间件介绍</title><link>https://jibinghu.github.io/post/C%2B%2B-zhong-jian-jian-jie-shao.html</link><description>以下是对RPC、Nginx、MongoDB、MQ和HAProxy的解释：&#13;
&#13;
### 1. RPC（Remote Procedure Call）&#13;
**RPC**是一种使程序能够在不同地址空间（通常在不同计算机上）调用彼此的方法的协议。</description><guid isPermaLink="true">https://jibinghu.github.io/post/C%2B%2B-zhong-jian-jian-jie-shao.html</guid><pubDate>Tue, 18 Jun 2024 01:30:47 +0000</pubDate></item><item><title>大话 Transformer(零基础看懂论文)</title><link>https://jibinghu.github.io/post/da-hua-%20Transformer%28-ling-ji-chu-kan-dong-lun-wen-%29.html</link><description>&#13;
---&#13;
&#13;
###### 由于课程实验要求以及专业学习关系，之前学过Transformer但仅局限于会用，这次深入探讨一下Transformer以便为大模型推理加速打个基础。</description><guid isPermaLink="true">https://jibinghu.github.io/post/da-hua-%20Transformer%28-ling-ji-chu-kan-dong-lun-wen-%29.html</guid><pubDate>Sat, 15 Jun 2024 06:20:07 +0000</pubDate></item><item><title>Brain Computer Interface</title><link>https://jibinghu.github.io/post/Brain%20Computer%20Interface.html</link><description>#### Technical terms learning:&#13;
&#13;
##### Brain-Computer Interface(BCI / 脑机接口):&#13;
&#13;
**定义：**&#13;
脑机接口是在大脑与外部设备之间创建信息通道，实现两者之间直接信息交互的新型交叉技术。</description><guid isPermaLink="true">https://jibinghu.github.io/post/Brain%20Computer%20Interface.html</guid><pubDate>Wed, 12 Jun 2024 13:36:26 +0000</pubDate></item><item><title>使用 ViT 训练 Cifar10 数据集</title><link>https://jibinghu.github.io/post/shi-yong-%20ViT%20-xun-lian-%20Cifar10%20-shu-ju-ji.html</link><description>#### 使用 ViT 训练 Cifar10 数据集&#13;
&#13;
**主要用来记录学习 ViT 中的问题帖子**&#13;
&#13;
---&#13;
&#13;
`nn.LayerNorm` 是 PyTorch 中的一个标准化层，它在神经网络中用于对输入数据进行层归一化。</description><guid isPermaLink="true">https://jibinghu.github.io/post/shi-yong-%20ViT%20-xun-lian-%20Cifar10%20-shu-ju-ji.html</guid><pubDate>Wed, 12 Jun 2024 10:27:08 +0000</pubDate></item><item><title>CUDA 矩阵乘优化分析</title><link>https://jibinghu.github.io/post/CUDA%20-ju-zhen-cheng-you-hua-fen-xi.html</link><description>#### 通过Shared Memory加速矩阵乘(Double等类型)分析&#13;
---&#13;
- [64位数据矩阵乘优化访存分析](#sector_1)&#13;
- [矩阵乘法的 CUDA 优化](#sector_2)&#13;
---&#13;
&#13;
#### 64位数据矩阵乘优化访存分析 {#sector_1}&#13;
&#13;
通过分析下面的代码，回答对应的两个问题(答案在文章结尾给出)。</description><guid isPermaLink="true">https://jibinghu.github.io/post/CUDA%20-ju-zhen-cheng-you-hua-fen-xi.html</guid><pubDate>Mon, 10 Jun 2024 12:45:09 +0000</pubDate></item><item><title>CUDA 线程布局以及内存层次</title><link>https://jibinghu.github.io/post/CUDA%20-xian-cheng-bu-ju-yi-ji-nei-cun-ceng-ci.html</link><description>### CUDA线程布局和内存层次&#13;
&#13;
&gt; [!CAUTION]&#13;
&gt; TODO :  CUDA 中 CUDA Core 硬件结构(SM/SP等)与软件层面布局对应关系及介绍&#13;
&#13;
**CUDA线程布局：**&#13;
&#13;
&lt;img src='https://img2024.cnblogs.com/blog/3358182/202405/3358182-20240514171810593-659841696.png' weight='300' height='200'&gt;&#13;
&#13;
如图所示，CUDA线程布局分为三层：网格(Grid),线程块(Block)以及线程(thread)&#13;
&#13;
&gt; [!IMPORTANT]&#13;
&gt; 在计算机中，内存的访问是一维的，线程的访问实质上也是一维的。</description><guid isPermaLink="true">https://jibinghu.github.io/post/CUDA%20-xian-cheng-bu-ju-yi-ji-nei-cun-ceng-ci.html</guid><pubDate>Wed, 05 Jun 2024 07:57:05 +0000</pubDate></item><item><title>Tensor core 详解</title><link>https://jibinghu.github.io/post/Tensor%20core%20-xiang-jie.html</link><description>## Tensor core 详解&#13;
&#13;
---&#13;
&#13;
#### Tensor Core剖析&#13;
&#13;
&gt; 在 NVIDIA 的通用 GPU 架构中，存在三种主要的核心类型：CUDA Core、Tensor Core 以及 RT Core。</description><guid isPermaLink="true">https://jibinghu.github.io/post/Tensor%20core%20-xiang-jie.html</guid><pubDate>Tue, 04 Jun 2024 13:52:36 +0000</pubDate></item><item><title>Java+微信小程序_Web介绍</title><link>https://jibinghu.github.io/post/Java%2B-wei-xin-xiao-cheng-xu-_Web-jie-shao.html</link><description>&gt; 前言：由于课程需要，简单地对 Java 相关框架以及Java Web相关知识做简单地学习，以备他用。</description><guid isPermaLink="true">https://jibinghu.github.io/post/Java%2B-wei-xin-xiao-cheng-xu-_Web-jie-shao.html</guid><pubDate>Mon, 03 Jun 2024 14:40:27 +0000</pubDate></item><item><title>从矩阵转置看共享内存(CUDA)</title><link>https://jibinghu.github.io/post/cong-ju-zhen-zhuan-zhi-kan-gong-xiang-nei-cun-%28CUDA%29.html</link><description>### 从矩阵转置看共享内存(CUDA的使用：Bank Conflict与Memory Coalesce)&#13;
---&#13;
- [矩阵转置的几种方法：](#sector_1)&#13;
  - [矩阵转置朴素实现：](#sector_1)&#13;
  - [利用共享内存合并访存：](#sector_2)&#13;
  - [利用 padding 解决 bank conflict：](#sector_3)&#13;
  - [增加每个线程的处理元素个数：](#sector_4)&#13;
  - [向量化存取：](#sector_5)&#13;
- [矩阵转置综合应用：](#chapter_2)&#13;
  - [Float数据类型转置：](#float)&#13;
  - [Double数据类型转置：](#double)&#13;
---&#13;
&gt; 矩阵转置是一种基础的矩阵操作, 即将二维矩阵的行列进行反转，本文主要围绕行主序的二维单精度矩阵的转置考虑相关的优化。</description><guid isPermaLink="true">https://jibinghu.github.io/post/cong-ju-zhen-zhuan-zhi-kan-gong-xiang-nei-cun-%28CUDA%29.html</guid><pubDate>Mon, 03 Jun 2024 03:41:48 +0000</pubDate></item><item><title>PaperReading_ConvStencil</title><link>https://jibinghu.github.io/post/PaperReading_ConvStencil.html</link><description>##### *PAPER READING*&#13;
&#13;
**@address: https://dl.acm.org/doi/10.1145/3627535.3638476**&#13;
**@github: https://github.com/microsoft/ConvStencil**&#13;
&#13;
### ConvStencil: Transform Stencil Computation to Matrix Multiplication on Tensor Cores&#13;
&#13;
##### 关键词：&#13;
    模版计算，卷积，张量核，矩阵乘&#13;
&#13;
##### 摘要：&#13;
&#13;
文章提出了ConvStencil，通过有效地将stencil模版计算转化为在张量核Tensor Core上的矩阵计算来实现。</description><guid isPermaLink="true">https://jibinghu.github.io/post/PaperReading_ConvStencil.html</guid><pubDate>Wed, 29 May 2024 11:24:12 +0000</pubDate></item><item><title>记录侯战森的罪证！</title><link>https://jibinghu.github.io/post/ji-lu-hou-zhan-sen-de-zui-zheng-%EF%BC%81.html</link><description>#### **天地可证**，侯战森是个大傻逼&#13;
&#13;
&lt;img src='https://img2024.cnblogs.com/blog/3358182/202405/3358182-20240528232629437-1616844405.jpg'&gt;。</description><guid isPermaLink="true">https://jibinghu.github.io/post/ji-lu-hou-zhan-sen-de-zui-zheng-%EF%BC%81.html</guid><pubDate>Tue, 28 May 2024 15:27:34 +0000</pubDate></item><item><title>new </title><link>https://jibinghu.github.io/post/new%20.html</link><description>first。</description><guid isPermaLink="true">https://jibinghu.github.io/post/new%20.html</guid><pubDate>Tue, 28 May 2024 10:09:39 +0000</pubDate></item></channel></rss>