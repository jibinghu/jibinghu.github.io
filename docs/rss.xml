<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>ZOMBIE_</title><link>https://jibinghu.github.io</link><description>我可能当不了绝世高手</description><copyright>ZOMBIE_</copyright><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://q6.itc.cn/q_70/images01/20240415/2cdb0abd9b724802baff3b9199d3fbc4.jpeg</url><title>avatar</title><link>https://jibinghu.github.io</link></image><lastBuildDate>Wed, 23 Oct 2024 14:14:19 +0000</lastBuildDate><managingEditor>ZOMBIE_</managingEditor><ttl>60</ttl><webMaster>ZOMBIE_</webMaster><item><title>ConvStencil: Transform Stencil Computation to Matrix Multiplication on Tensor Cores</title><link>https://jibinghu.github.io/post/ConvStencil-%20Transform%20Stencil%20Computation%20to%20Matrix%20Multiplication%20on%20Tensor%20Cores.html</link><description>ConvStencil: Transform Stencil Computation to Matrix Multiplication on Tensor Cores&#13;
&#13;
链接：https://dl.acm.org/doi/pdf/10.1145/3627535.3638476&#13;
引用：Yuetao Chen, Kun Li, Yuhao Wang, Donglin Bai, Lei Wang, Lingxiao Ma, Liang Yuan, Yunquan Zhang, Ting Cao, and Mao Yang. 2024. ConvStencil: Transform Stencil Computation to Matrix Multiplication on Tensor Cores. In Proceedings of the 29th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming (PPoPP '24). Association for Computing Machinery, New York, NY, USA, 333–347. https://doi.org/10.1145/3627535.3638476&#13;
&#13;
---&#13;
&#13;
不要钻牛角尖，细节的东西暂时不需要花大把时间去考虑。</description><guid isPermaLink="true">https://jibinghu.github.io/post/ConvStencil-%20Transform%20Stencil%20Computation%20to%20Matrix%20Multiplication%20on%20Tensor%20Cores.html</guid><pubDate>Mon, 21 Oct 2024 14:50:32 +0000</pubDate></item><item><title>分组卷积</title><link>https://jibinghu.github.io/post/fen-zu-juan-ji.html</link><description>&lt;a href='https://paddlepedia.readthedocs.io/en/latest/tutorials/CNN/convolution_operator/Group_Convolution.html'&gt;PaddlePaddle深度学习知识&lt;/a&gt;&#13;
---&#13;
### 分组卷积&#13;
&#13;
对于尺寸为 𝐻1×𝑊1×𝐶1&#13;
 的输入矩阵，当标准卷积核的尺寸为 ℎ1×𝑤1×𝐶1&#13;
 ，共有 𝐶2&#13;
 个标准卷积核时，标准卷积会对完整的输入数据进行运算，最终得到的输出矩阵尺寸为 𝐻2×𝑊2×𝐶2&#13;
 。</description><guid isPermaLink="true">https://jibinghu.github.io/post/fen-zu-juan-ji.html</guid><pubDate>Mon, 21 Oct 2024 04:09:57 +0000</pubDate></item><item><title>C++智能指针</title><link>https://jibinghu.github.io/post/C%2B%2B-zhi-neng-zhi-zhen.html</link><description>智能指针是C++中的一种用于自动管理动态内存的指针，它们能够自动释放不再使用的对象，避免内存泄漏。</description><guid isPermaLink="true">https://jibinghu.github.io/post/C%2B%2B-zhi-neng-zhi-zhen.html</guid><pubDate>Thu, 17 Oct 2024 06:05:51 +0000</pubDate></item><item><title>每日翻译！</title><link>https://jibinghu.github.io/post/mei-ri-fan-yi-%EF%BC%81.html</link><description>文言文：&#13;
- 行当务之事，亦宜分神以探未来之道，扩展格局。</description><guid isPermaLink="true">https://jibinghu.github.io/post/mei-ri-fan-yi-%EF%BC%81.html</guid><pubDate>Tue, 15 Oct 2024 13:54:21 +0000</pubDate></item><item><title>Pooling 层 -&gt; TEST</title><link>https://jibinghu.github.io/post/Pooling%20-ceng-%20--%20TEST.html</link><description>基于 Torch 的脚本：&#13;
``` python&#13;
import torch&#13;
import torch.nn as nn&#13;
&#13;
# Create a sample tensor (2D)&#13;
input_tensor = torch.tensor([[1., 2., 3., 4.],&#13;
                             [5., 6., 7., 8.],&#13;
                             [9., 10., 11., 12.],&#13;
                             [13., 14., 15., 16.]])&#13;
&#13;
# Reshape the tensor to 1x1x4x4 (as expected by pooling layers for 2D input)&#13;
input_tensor = input_tensor.unsqueeze(0).unsqueeze(0)&#13;
&#13;
# Max Pooling with padding&#13;
max_pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)&#13;
max_pooled_output = max_pool(input_tensor)&#13;
&#13;
# Average Pooling with padding&#13;
# avg_pool = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)&#13;
# avg_pooled_output = avg_pool(input_tensor)&#13;
&#13;
print('Input Tensor:\n', input_tensor)&#13;
print('\nMax Pooled Output with Padding:\n', max_pooled_output)&#13;
print('\nAverage Pooled Output with Padding:\n', avg_pooled_output)&#13;
```&#13;
&#13;
---&#13;
&#13;
### 4 * 4 的 Pooling 层是比较典型的，所以进行多方面测试：&#13;
&#13;
- **kernel_size=2, stride=2, padding=0：**&#13;
``` bash&#13;
Max Pooled Output with Padding:&#13;
 tensor([[[[ 6.,  7.,  8.],&#13;
          [10., 11., 12.],&#13;
          [14., 15., 16.]]]])&#13;
```&#13;
这部分还是没有异议的，当Stride=1时从开头进行Pooling；&#13;
- **kernel_size=2, stride=2, padding=0：**&#13;
``` bash&#13;
 tensor([[[[ 6.,  8.],&#13;
          [14., 16.]]]])&#13;
```&#13;
当Stride足够覆盖一次时，也是没有异议的。</description><guid isPermaLink="true">https://jibinghu.github.io/post/Pooling%20-ceng-%20--%20TEST.html</guid><pubDate>Tue, 15 Oct 2024 13:42:40 +0000</pubDate></item><item><title>每日翻译！</title><link>https://jibinghu.github.io/post/mei-ri-fan-yi-%EF%BC%81.html</link><description>文言文：&#13;
&#13;
持志如焚，持行如矢，志不改，行不止。</description><guid isPermaLink="true">https://jibinghu.github.io/post/mei-ri-fan-yi-%EF%BC%81.html</guid><pubDate>Mon, 14 Oct 2024 14:56:47 +0000</pubDate></item><item><title>C++设计模式之单例模式与工厂模式</title><link>https://jibinghu.github.io/post/C%2B%2B-she-ji-mo-shi-zhi-dan-li-mo-shi-yu-gong-chang-mo-shi.html</link><description>1. 单例模式（Singleton Pattern）&#13;
&#13;
单例模式是一种设计模式，保证在应用程序的生命周期内，一个类只有一个实例，并且提供一个全局访问点来获取这个实例。</description><guid isPermaLink="true">https://jibinghu.github.io/post/C%2B%2B-she-ji-mo-shi-zhi-dan-li-mo-shi-yu-gong-chang-mo-shi.html</guid><pubDate>Mon, 14 Oct 2024 07:42:14 +0000</pubDate></item><item><title>C++结构化绑定</title><link>https://jibinghu.github.io/post/C%2B%2B-jie-gou-hua-bang-ding.html</link><description>## C++ 17 结构化绑定&#13;
&#13;
stl 的 map 容器很多读者应该都很熟悉，map 容器提供了一个 **insert** 方法，我们用该方法向 map 中插入元素，但是应该很少有人记得 **insert** 方法的返回值是什么类型，让我们来看一下 C++98/03 提供的 **insert** 方法的签名：&#13;
&#13;
```&#13;
std::pair&lt;iterator,bool&gt; insert( const value_type&amp; value );&#13;
```&#13;
&#13;
这里我们仅关心其返回值，这个返回值是一个 **std::pair** 类型，由于 map 中的元素的 key 不允许重复，所以如果 insert 方法调用成功，T1 是被成功插入到 map 中的元素的迭代器，T2 的类型为 bool，此时其值为 true（表示插入成功）；如果 insert 由于 key 重复，T1 是造成 insert 插入失败、已经存在于 map 中的元素的迭代器，此时 T2 的值为 false（表示插入失败）。</description><guid isPermaLink="true">https://jibinghu.github.io/post/C%2B%2B-jie-gou-hua-bang-ding.html</guid><pubDate>Fri, 11 Oct 2024 05:24:46 +0000</pubDate></item><item><title>`GLIBCXX_3.4.32' not found" error at runtime. GCC 13.2.0 问题的解决方式：StackOverflow</title><link>https://jibinghu.github.io/post/%60GLIBCXX_3.4.32%27%20not%20found-%20error%20at%20runtime.%20GCC%2013.2.0%20-wen-ti-de-jie-jue-fang-shi-%EF%BC%9AStackOverflow.html</link><description>&gt; the /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' is for gcc13, so we need to update this file.`&#13;
&#13;
0. 查看当前 GLIBCXX 版本&#13;
首先通过命令`strings /usr/lib/x86_64-linux-gnu/libstdc++.so.6 | grep GLIBCXX`&#13;
- strings：这是一个 Linux 命令，用于提取二进制文件或库文件中可打印的字符串。</description><guid isPermaLink="true">https://jibinghu.github.io/post/%60GLIBCXX_3.4.32%27%20not%20found-%20error%20at%20runtime.%20GCC%2013.2.0%20-wen-ti-de-jie-jue-fang-shi-%EF%BC%9AStackOverflow.html</guid><pubDate>Thu, 10 Oct 2024 13:27:18 +0000</pubDate></item><item><title>C++ 中常用的检查宏：CHECK系列 -&gt; 标准库/glog库 &lt;&lt; 单元测试；std::function </title><link>https://jibinghu.github.io/post/C%2B%2B%20-zhong-chang-yong-de-jian-cha-hong-%EF%BC%9ACHECK-xi-lie-%20--%20-biao-zhun-ku--glog-ku-%20--%20-dan-yuan-ce-shi-%EF%BC%9Bstd--function%20.html</link><description>在C++中，类似于`CHECK_LT`的断言方法通常用于验证条件，并在条件不满足时触发错误或异常。</description><guid isPermaLink="true">https://jibinghu.github.io/post/C%2B%2B%20-zhong-chang-yong-de-jian-cha-hong-%EF%BC%9ACHECK-xi-lie-%20--%20-biao-zhun-ku--glog-ku-%20--%20-dan-yuan-ce-shi-%EF%BC%9Bstd--function%20.html</guid><pubDate>Wed, 09 Oct 2024 08:06:20 +0000</pubDate></item><item><title>类模板的特化 #举例</title><link>https://jibinghu.github.io/post/lei-mo-ban-de-te-hua-%20%23-ju-li.html</link><description>### 例子：通用 `Array` 类和 `bool` 类型的特化&#13;
&#13;
创建一个通用的 `Array` 类模板，它可以存储任意类型的数据。</description><guid isPermaLink="true">https://jibinghu.github.io/post/lei-mo-ban-de-te-hua-%20%23-ju-li.html</guid><pubDate>Wed, 09 Oct 2024 03:38:54 +0000</pubDate></item><item><title>单目深度估计</title><link>https://jibinghu.github.io/post/dan-mu-shen-du-gu-ji.html</link><description>**单目深度估计**（Monocular Depth Estimation）是计算机视觉中的一个经典任务，目标是通过一张单目摄像头（即仅包含一个视角的二维图像）来估计场景中每个像素的深度信息。</description><guid isPermaLink="true">https://jibinghu.github.io/post/dan-mu-shen-du-gu-ji.html</guid><pubDate>Mon, 09 Sep 2024 07:37:00 +0000</pubDate></item><item><title>ONNX配置参数说明（v1）</title><link>https://jibinghu.github.io/post/ONNX-pei-zhi-can-shu-shuo-ming-%EF%BC%88v1%EF%BC%89.html</link><description>ONNX配置参数说明（v1）&#13;
&#13;
torch Version 2.1.0 的 export参数：&#13;
``` python&#13;
def export(&#13;
    model: Union[torch.nn.Module, torch.jit.ScriptModule, torch.jit.ScriptFunction],&#13;
    args: Union[Tuple[Any, ...], torch.Tensor],&#13;
    f: Union[str, io.BytesIO],&#13;
    export_params: bool = True,&#13;
    verbose: bool = False,&#13;
    training: _C_onnx.TrainingMode = _C_onnx.TrainingMode.EVAL,&#13;
    input_names: Optional[Sequence[str]] = None,&#13;
    output_names: Optional[Sequence[str]] = None,&#13;
    operator_export_type: _C_onnx.OperatorExportTypes = _C_onnx.OperatorExportTypes.ONNX,&#13;
    opset_version: Optional[int] = None,&#13;
    do_constant_folding: bool = True,&#13;
    dynamic_axes: Optional[&#13;
        Union[Mapping[str, Mapping[int, str]], Mapping[str, Sequence[int]]]&#13;
    ] = None,&#13;
    keep_initializers_as_inputs: Optional[bool] = None,&#13;
    custom_opsets: Optional[Mapping[str, int]] = None,&#13;
    export_modules_as_functions: Union[bool, Collection[Type[torch.nn.Module]]] = False,&#13;
    autograd_inlining: Optional[bool] = True,&#13;
)&#13;
```&#13;
&#13;
model: pytorch模型&#13;
args: 第一个参数model的输入数据，因为模型的输入可能不止一个，因此采用元组作为参数&#13;
export_params: 导出的onnx模型文件可以包含网络结构与权重参数，如果设置该参数为False，则导出的onnx模型文件只包含网络结构，因此，一般保持默认为True即可&#13;
verbose: 该参数如果指定为True，则在导出onnx的过程中会打印详细的导出过程信息&#13;
&#13;
opset_version: ONNX的算子集版本，默认为11。</description><guid isPermaLink="true">https://jibinghu.github.io/post/ONNX-pei-zhi-can-shu-shuo-ming-%EF%BC%88v1%EF%BC%89.html</guid><pubDate>Tue, 03 Sep 2024 13:54:39 +0000</pubDate></item><item><title>CUDA 编程模型中的 Block 的共享内存与 SM 的L1 Cache和Shared Memory</title><link>https://jibinghu.github.io/post/CUDA%20-bian-cheng-mo-xing-zhong-de-%20Block%20-de-gong-xiang-nei-cun-yu-%20SM%20-de-L1%20Cache-he-Shared%20Memory.html</link><description>### CUDA 编程模型中的 Block 的共享内存与 SM 的L1 Cache和Shared Memory有什么区别和联系？&#13;
&#13;
在 CUDA 编程模型中，Block 的共享内存（Shared Memory）与 SM（Streaming Multiprocessor）的 L1 Cache 和 Shared Memory 是两个重要的内存层级，它们在用途、性能和实现上都有所不同。</description><guid isPermaLink="true">https://jibinghu.github.io/post/CUDA%20-bian-cheng-mo-xing-zhong-de-%20Block%20-de-gong-xiang-nei-cun-yu-%20SM%20-de-L1%20Cache-he-Shared%20Memory.html</guid><pubDate>Mon, 02 Sep 2024 06:06:48 +0000</pubDate></item><item><title>NV知识库(SASS和PTX中间代码)</title><link>https://jibinghu.github.io/post/NV-zhi-shi-ku-%28SASS-he-PTX-zhong-jian-dai-ma-%29.html</link><description>### SASS 和 PTX&#13;
&#13;
SASS(Streaming Assembler) 和 PTX(Parallel Thread Execution)都是 NVIDIA CUDA 编程模型中的组件，处于不同的抽象层次。</description><guid isPermaLink="true">https://jibinghu.github.io/post/NV-zhi-shi-ku-%28SASS-he-PTX-zhong-jian-dai-ma-%29.html</guid><pubDate>Wed, 28 Aug 2024 10:11:47 +0000</pubDate></item><item><title> How_to_optimize_in_GPU_GEMM_(二)</title><link>https://jibinghu.github.io/post/%20How_to_optimize_in_GPU_GEMM_%28-er-%29.html</link><description>&lt;a href='https://github.com/Liu-xiandong/How_to_optimize_in_GPU'&gt; How_to_optimize_in_GPU_GEMM_(二)_评论分析&lt;/a&gt;&#13;
---&#13;
你好想问一下看起来并没有用异步的指令为什么可以实现数据预取呢&#13;
&gt; pipeline 双缓冲 pingpong操作，一个事情，都是为了实现计算和访存错开。</description><guid isPermaLink="true">https://jibinghu.github.io/post/%20How_to_optimize_in_GPU_GEMM_%28-er-%29.html</guid><pubDate>Sun, 25 Aug 2024 14:50:34 +0000</pubDate></item><item><title>海光 DCU 相关知识</title><link>https://jibinghu.github.io/post/hai-guang-%20DCU%20-xiang-guan-zhi-shi.html</link><description>&gt; 目前还是在学习阶段，把之后可能时常需要用到的技术备忘在这里。</description><guid isPermaLink="true">https://jibinghu.github.io/post/hai-guang-%20DCU%20-xiang-guan-zhi-shi.html</guid><pubDate>Fri, 23 Aug 2024 14:51:13 +0000</pubDate></item><item><title>C++模板的使用</title><link>https://jibinghu.github.io/post/C%2B%2B-mo-ban-de-shi-yong.html</link><description>模板是C++支持[参数化](https://so.csdn.net/so/search?q=%E5%8F%82%E6%95%B0%E5%8C%96&amp;spm=1001.2101.3001.7020)多态的工具，模板的参数有三种类型：类型参数、非类型参数和模板类型参数。</description><guid isPermaLink="true">https://jibinghu.github.io/post/C%2B%2B-mo-ban-de-shi-yong.html</guid><pubDate>Fri, 23 Aug 2024 07:46:25 +0000</pubDate></item><item><title>AWQ量化</title><link>https://jibinghu.github.io/post/AWQ-liang-hua.html</link><description>挑选显著权重：权重矩阵的一行作为一个单位。</description><guid isPermaLink="true">https://jibinghu.github.io/post/AWQ-liang-hua.html</guid><pubDate>Wed, 21 Aug 2024 09:51:57 +0000</pubDate></item><item><title>基座模型私有数据训练</title><link>https://jibinghu.github.io/post/ji-zuo-mo-xing-si-you-shu-ju-xun-lian.html</link><description>针对基座模型（例如大型语言模型）进行私有数据训练，以下是几种代价较小的方式：&#13;
&#13;
1. QLoRA 微调&#13;
- 概念：QLoRA（Quantized Low Rank Adaptation）是一种利用低秩矩阵分解和量化技术的微调方法，能够在模型参数显著减少的情况下，实现类似全量模型微调的效果。</description><guid isPermaLink="true">https://jibinghu.github.io/post/ji-zuo-mo-xing-si-you-shu-ju-xun-lian.html</guid><pubDate>Wed, 21 Aug 2024 09:50:57 +0000</pubDate></item><item><title>Pre-Norm&amp;Post-Norm</title><link>https://jibinghu.github.io/post/Pre-Norm%26Post-Norm.html</link><description>![](https://img2024.cnblogs.com/blog/3358182/202407/3358182-20240717110533959-1175740727.png)&#13;
&#13;
&#13;
从图中可以看出，两种不同的Transformer结构：Post-Norm Residual Unit 和 Pre-Norm Residual Unit。</description><guid isPermaLink="true">https://jibinghu.github.io/post/Pre-Norm%26Post-Norm.html</guid><pubDate>Wed, 17 Jul 2024 03:07:42 +0000</pubDate></item><item><title>CUDA binary analysis utils</title><link>https://jibinghu.github.io/post/CUDA%20binary%20analysis%20utils.html</link><description>cuobjdump ：&#13;
cuobjdump 是 NVIDIA 提供的一个工具，用于提取和显示 CUDA 二进制文件（即 CUDA 应用程序的可执行文件）中的信，可以用来分析cubin文件和host文件。</description><guid isPermaLink="true">https://jibinghu.github.io/post/CUDA%20binary%20analysis%20utils.html</guid><pubDate>Tue, 16 Jul 2024 08:21:47 +0000</pubDate></item><item><title>C++中间件介绍</title><link>https://jibinghu.github.io/post/C%2B%2B-zhong-jian-jian-jie-shao.html</link><description>以下是对RPC、Nginx、MongoDB、MQ和HAProxy的解释：&#13;
&#13;
### 1. RPC（Remote Procedure Call）&#13;
**RPC**是一种使程序能够在不同地址空间（通常在不同计算机上）调用彼此的方法的协议。</description><guid isPermaLink="true">https://jibinghu.github.io/post/C%2B%2B-zhong-jian-jian-jie-shao.html</guid><pubDate>Tue, 18 Jun 2024 01:30:47 +0000</pubDate></item><item><title>大话 Transformer(零基础看懂论文)</title><link>https://jibinghu.github.io/post/da-hua-%20Transformer%28-ling-ji-chu-kan-dong-lun-wen-%29.html</link><description>&#13;
---&#13;
&#13;
###### 由于课程实验要求以及专业学习关系，之前学过Transformer但仅局限于会用，这次深入探讨一下Transformer以便为大模型推理加速打个基础。</description><guid isPermaLink="true">https://jibinghu.github.io/post/da-hua-%20Transformer%28-ling-ji-chu-kan-dong-lun-wen-%29.html</guid><pubDate>Sat, 15 Jun 2024 06:20:07 +0000</pubDate></item><item><title>Brain Computer Interface</title><link>https://jibinghu.github.io/post/Brain%20Computer%20Interface.html</link><description>#### Technical terms learning:&#13;
&#13;
##### Brain-Computer Interface(BCI / 脑机接口):&#13;
&#13;
**定义：**&#13;
脑机接口是在大脑与外部设备之间创建信息通道，实现两者之间直接信息交互的新型交叉技术。</description><guid isPermaLink="true">https://jibinghu.github.io/post/Brain%20Computer%20Interface.html</guid><pubDate>Wed, 12 Jun 2024 13:36:26 +0000</pubDate></item><item><title>使用 ViT 训练 Cifar10 数据集</title><link>https://jibinghu.github.io/post/shi-yong-%20ViT%20-xun-lian-%20Cifar10%20-shu-ju-ji.html</link><description>#### 使用 ViT 训练 Cifar10 数据集&#13;
&#13;
**主要用来记录学习 ViT 中的问题帖子**&#13;
&#13;
---&#13;
&#13;
`nn.LayerNorm` 是 PyTorch 中的一个标准化层，它在神经网络中用于对输入数据进行层归一化。</description><guid isPermaLink="true">https://jibinghu.github.io/post/shi-yong-%20ViT%20-xun-lian-%20Cifar10%20-shu-ju-ji.html</guid><pubDate>Wed, 12 Jun 2024 10:27:08 +0000</pubDate></item><item><title>CUDA 矩阵乘优化分析</title><link>https://jibinghu.github.io/post/CUDA%20-ju-zhen-cheng-you-hua-fen-xi.html</link><description>#### 通过Shared Memory加速矩阵乘(Double等类型)分析&#13;
---&#13;
- [64位数据矩阵乘优化访存分析](#sector_1)&#13;
- [矩阵乘法的 CUDA 优化](#sector_2)&#13;
---&#13;
&#13;
#### 64位数据矩阵乘优化访存分析 {#sector_1}&#13;
&#13;
通过分析下面的代码，回答对应的两个问题(答案在文章结尾给出)。</description><guid isPermaLink="true">https://jibinghu.github.io/post/CUDA%20-ju-zhen-cheng-you-hua-fen-xi.html</guid><pubDate>Mon, 10 Jun 2024 12:45:09 +0000</pubDate></item><item><title>CUDA 线程布局以及内存层次</title><link>https://jibinghu.github.io/post/CUDA%20-xian-cheng-bu-ju-yi-ji-nei-cun-ceng-ci.html</link><description>### CUDA线程布局和内存层次&#13;
&#13;
&gt; [!CAUTION]&#13;
&gt; TODO :  CUDA 中 CUDA Core 硬件结构(SM/SP等)与软件层面布局对应关系及介绍&#13;
&#13;
**CUDA线程布局：**&#13;
&#13;
&lt;img src='https://img2024.cnblogs.com/blog/3358182/202405/3358182-20240514171810593-659841696.png' weight='300' height='200'&gt;&#13;
&#13;
如图所示，CUDA线程布局分为三层：网格(Grid),线程块(Block)以及线程(thread)&#13;
&#13;
&gt; [!IMPORTANT]&#13;
&gt; 在计算机中，内存的访问是一维的，线程的访问实质上也是一维的。</description><guid isPermaLink="true">https://jibinghu.github.io/post/CUDA%20-xian-cheng-bu-ju-yi-ji-nei-cun-ceng-ci.html</guid><pubDate>Wed, 05 Jun 2024 07:57:05 +0000</pubDate></item><item><title>Tensor core 详解</title><link>https://jibinghu.github.io/post/Tensor%20core%20-xiang-jie.html</link><description>## Tensor core 详解&#13;
&#13;
---&#13;
&#13;
#### Tensor Core剖析&#13;
&#13;
&gt; 在 NVIDIA 的通用 GPU 架构中，存在三种主要的核心类型：CUDA Core、Tensor Core 以及 RT Core。</description><guid isPermaLink="true">https://jibinghu.github.io/post/Tensor%20core%20-xiang-jie.html</guid><pubDate>Tue, 04 Jun 2024 13:52:36 +0000</pubDate></item><item><title>Java+微信小程序_Web介绍</title><link>https://jibinghu.github.io/post/Java%2B-wei-xin-xiao-cheng-xu-_Web-jie-shao.html</link><description>&gt; 前言：由于课程需要，简单地对 Java 相关框架以及Java Web相关知识做简单地学习，以备他用。</description><guid isPermaLink="true">https://jibinghu.github.io/post/Java%2B-wei-xin-xiao-cheng-xu-_Web-jie-shao.html</guid><pubDate>Mon, 03 Jun 2024 14:40:27 +0000</pubDate></item><item><title>从矩阵转置看共享内存(CUDA)</title><link>https://jibinghu.github.io/post/cong-ju-zhen-zhuan-zhi-kan-gong-xiang-nei-cun-%28CUDA%29.html</link><description>### 从矩阵转置看共享内存(CUDA的使用：Bank Conflict与Memory Coalesce)&#13;
---&#13;
- [矩阵转置的几种方法：](#sector_1)&#13;
  - [矩阵转置朴素实现：](#sector_1)&#13;
  - [利用共享内存合并访存：](#sector_2)&#13;
  - [利用 padding 解决 bank conflict：](#sector_3)&#13;
  - [增加每个线程的处理元素个数：](#sector_4)&#13;
  - [向量化存取：](#sector_5)&#13;
- [矩阵转置综合应用：](#chapter_2)&#13;
  - [Float数据类型转置：](#float)&#13;
  - [Double数据类型转置：](#double)&#13;
---&#13;
&gt; 矩阵转置是一种基础的矩阵操作, 即将二维矩阵的行列进行反转，本文主要围绕行主序的二维单精度矩阵的转置考虑相关的优化。</description><guid isPermaLink="true">https://jibinghu.github.io/post/cong-ju-zhen-zhuan-zhi-kan-gong-xiang-nei-cun-%28CUDA%29.html</guid><pubDate>Mon, 03 Jun 2024 03:41:48 +0000</pubDate></item><item><title>PaperReading_ConvStencil</title><link>https://jibinghu.github.io/post/PaperReading_ConvStencil.html</link><description>##### *PAPER READING*&#13;
&#13;
**@address: https://dl.acm.org/doi/10.1145/3627535.3638476**&#13;
**@github: https://github.com/microsoft/ConvStencil**&#13;
&#13;
### ConvStencil: Transform Stencil Computation to Matrix Multiplication on Tensor Cores&#13;
&#13;
##### 关键词：&#13;
    模版计算，卷积，张量核，矩阵乘&#13;
&#13;
##### 摘要：&#13;
&#13;
文章提出了ConvStencil，通过有效地将stencil模版计算转化为在张量核Tensor Core上的矩阵计算来实现。</description><guid isPermaLink="true">https://jibinghu.github.io/post/PaperReading_ConvStencil.html</guid><pubDate>Wed, 29 May 2024 11:24:12 +0000</pubDate></item><item><title>记录侯战森的罪证！</title><link>https://jibinghu.github.io/post/ji-lu-hou-zhan-sen-de-zui-zheng-%EF%BC%81.html</link><description>#### **天地可证**，侯战森是个大傻逼&#13;
&#13;
&lt;img src='https://img2024.cnblogs.com/blog/3358182/202405/3358182-20240528232629437-1616844405.jpg'&gt;。</description><guid isPermaLink="true">https://jibinghu.github.io/post/ji-lu-hou-zhan-sen-de-zui-zheng-%EF%BC%81.html</guid><pubDate>Tue, 28 May 2024 15:27:34 +0000</pubDate></item><item><title>new </title><link>https://jibinghu.github.io/post/new%20.html</link><description>first。</description><guid isPermaLink="true">https://jibinghu.github.io/post/new%20.html</guid><pubDate>Tue, 28 May 2024 10:09:39 +0000</pubDate></item></channel></rss>