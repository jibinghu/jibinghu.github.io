<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark_high_contrast" data-light-theme="light_high_contrast" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    
    <link rel="icon" href="https://q6.itc.cn/q_70/images01/20240415/2cdb0abd9b724802baff3b9199d3fbc4.jpeg"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="``` python 
import json
import pandas as pd
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import os
import swanlab

# 设置 SwanLab 项目名称
os.environ['SWANLAB_PROJECT'] = 'qwen3-sft-dialog'

# 定义提示（PROMPT）和最大序列长度
PROMPT = '你是一个对话助手，你需要根据用户的问题，给出相应的回答。">
<meta property="og:title" content="全参数训练qwen3-0.5b">
<meta property="og:description" content="``` python 
import json
import pandas as pd
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq
import os
import swanlab

# 设置 SwanLab 项目名称
os.environ['SWANLAB_PROJECT'] = 'qwen3-sft-dialog'

# 定义提示（PROMPT）和最大序列长度
PROMPT = '你是一个对话助手，你需要根据用户的问题，给出相应的回答。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://jibinghu.github.io/post/quan-can-shu-xun-lian-qwen3-0.5b.html">
<meta property="og:image" content="https://q6.itc.cn/q_70/images01/20240415/2cdb0abd9b724802baff3b9199d3fbc4.jpeg">
<title>全参数训练qwen3-0.5b</title>
<link href="//unpkg.com/@wooorm/starry-night@2.1.1/style/both.css" rel="stylesheet" />


</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}
.copy-feedback {
    display: none;
    position: absolute;
    top: 10px;
    right: 50px;
    color: var(--color-fg-on-emphasis);
    background-color: var(--color-fg-muted);
    border-radius: 3px;
    padding: 5px 8px;
    font-size: 12px;
}
</style>




<body>
    <div id="header">
<h1 class="postTitle">全参数训练qwen3-0.5b</h1>
<div class="title-right">
    <a href="https://jibinghu.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/jibinghu/jibinghu.github.io/issues/155" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">json</span>
<span class="pl-k">import</span> <span class="pl-s1">pandas</span> <span class="pl-k">as</span> <span class="pl-s1">pd</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">datasets</span> <span class="pl-k">import</span> <span class="pl-v">Dataset</span>
<span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-v">AutoTokenizer</span>, <span class="pl-v">AutoModelForCausalLM</span>, <span class="pl-v">TrainingArguments</span>, <span class="pl-v">Trainer</span>, <span class="pl-v">DataCollatorForSeq2Seq</span>
<span class="pl-k">import</span> <span class="pl-s1">os</span>
<span class="pl-k">import</span> <span class="pl-s1">swanlab</span>

<span class="pl-c"># 设置 SwanLab 项目名称</span>
<span class="pl-s1">os</span>.<span class="pl-c1">environ</span>[<span class="pl-s">"SWANLAB_PROJECT"</span>] <span class="pl-c1">=</span> <span class="pl-s">"qwen3-sft-dialog"</span>

<span class="pl-c"># 定义提示（PROMPT）和最大序列长度</span>
<span class="pl-c1">PROMPT</span> <span class="pl-c1">=</span> <span class="pl-s">"你是一个对话助手，你需要根据用户的问题，给出相应的回答。"</span>
<span class="pl-c1">MAX_LENGTH</span> <span class="pl-c1">=</span> <span class="pl-c1">2048</span>

<span class="pl-c"># 更新 SwanLab 配置</span>
<span class="pl-s1">swanlab</span>.<span class="pl-c1">config</span>.<span class="pl-c1">update</span>({
    <span class="pl-s">"model"</span>: <span class="pl-s">"Qwen/Qwen3-0.5B"</span>,
    <span class="pl-s">"prompt"</span>: <span class="pl-c1">PROMPT</span>,
    <span class="pl-s">"data_max_length"</span>: <span class="pl-c1">MAX_LENGTH</span>,
})

<span class="pl-c"># 数据集格式转换函数</span>
<span class="pl-k">def</span> <span class="pl-en">dataset_jsonl_transfer</span>(<span class="pl-s1">origin_path</span>, <span class="pl-s1">new_path</span>):
    <span class="pl-s1">messages</span> <span class="pl-c1">=</span> []
    <span class="pl-k">with</span> <span class="pl-en">open</span>(<span class="pl-s1">origin_path</span>, <span class="pl-s">"r"</span>) <span class="pl-k">as</span> <span class="pl-s1">file</span>:
        <span class="pl-k">for</span> <span class="pl-s1">line</span> <span class="pl-c1">in</span> <span class="pl-s1">file</span>:
            <span class="pl-s1">data</span> <span class="pl-c1">=</span> <span class="pl-s1">json</span>.<span class="pl-c1">loads</span>(<span class="pl-s1">line</span>)
            <span class="pl-s1">message</span> <span class="pl-c1">=</span> {
                <span class="pl-s">"instruction"</span>: <span class="pl-c1">PROMPT</span>,
                <span class="pl-s">"input"</span>: <span class="pl-s1">data</span>[<span class="pl-s">"question"</span>],
                <span class="pl-s">"output"</span>: <span class="pl-s1">data</span>[<span class="pl-s">"answer"</span>],
            }
            <span class="pl-s1">messages</span>.<span class="pl-c1">append</span>(<span class="pl-s1">message</span>)
    <span class="pl-k">with</span> <span class="pl-en">open</span>(<span class="pl-s1">new_path</span>, <span class="pl-s">"w"</span>, <span class="pl-s1">encoding</span><span class="pl-c1">=</span><span class="pl-s">"utf-8"</span>) <span class="pl-k">as</span> <span class="pl-s1">file</span>:
        <span class="pl-k">for</span> <span class="pl-s1">message</span> <span class="pl-c1">in</span> <span class="pl-s1">messages</span>:
            <span class="pl-s1">file</span>.<span class="pl-c1">write</span>(<span class="pl-s1">json</span>.<span class="pl-c1">dumps</span>(<span class="pl-s1">message</span>, <span class="pl-s1">ensure_ascii</span><span class="pl-c1">=</span><span class="pl-c1">False</span>) <span class="pl-c1">+</span> <span class="pl-s">"<span class="pl-cce">\n</span>"</span>)

<span class="pl-c"># 数据预处理函数</span>
<span class="pl-k">def</span> <span class="pl-en">process_func</span>(<span class="pl-s1">example</span>):
    <span class="pl-s1">input_ids</span>, <span class="pl-s1">attention_mask</span>, <span class="pl-s1">labels</span> <span class="pl-c1">=</span> [], [], []
    <span class="pl-s1">instruction</span> <span class="pl-c1">=</span> <span class="pl-en">tokenizer</span>(
        <span class="pl-s">f"&lt;|im_start|&gt;system<span class="pl-cce">\n</span><span class="pl-s1"><span class="pl-kos">{</span><span class="pl-c1">PROMPT</span><span class="pl-kos">}</span></span>&lt;|im_end|&gt;<span class="pl-cce">\n</span>&lt;|im_start|&gt;user<span class="pl-cce">\n</span><span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">example</span>[<span class="pl-s">'input'</span>]<span class="pl-kos">}</span></span>&lt;|im_end|&gt;<span class="pl-cce">\n</span>&lt;|im_start|&gt;assistant<span class="pl-cce">\n</span>"</span>,
        <span class="pl-s1">add_special_tokens</span><span class="pl-c1">=</span><span class="pl-c1">False</span>,
    )
    <span class="pl-s1">response</span> <span class="pl-c1">=</span> <span class="pl-en">tokenizer</span>(<span class="pl-s">f"<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">example</span>[<span class="pl-s">'output'</span>]<span class="pl-kos">}</span></span>"</span>, <span class="pl-s1">add_special_tokens</span><span class="pl-c1">=</span><span class="pl-c1">False</span>)
    <span class="pl-s1">input_ids</span> <span class="pl-c1">=</span> <span class="pl-s1">instruction</span>[<span class="pl-s">"input_ids"</span>] <span class="pl-c1">+</span> <span class="pl-s1">response</span>[<span class="pl-s">"input_ids"</span>] <span class="pl-c1">+</span> [<span class="pl-s1">tokenizer</span>.<span class="pl-c1">pad_token_id</span>]
    <span class="pl-s1">attention_mask</span> <span class="pl-c1">=</span> <span class="pl-s1">instruction</span>[<span class="pl-s">"attention_mask"</span>] <span class="pl-c1">+</span> <span class="pl-s1">response</span>[<span class="pl-s">"attention_mask"</span>] <span class="pl-c1">+</span> [<span class="pl-c1">1</span>]
    <span class="pl-s1">labels</span> <span class="pl-c1">=</span> [<span class="pl-c1">-</span><span class="pl-c1">100</span>] <span class="pl-c1">*</span> <span class="pl-en">len</span>(<span class="pl-s1">instruction</span>[<span class="pl-s">"input_ids"</span>]) <span class="pl-c1">+</span> <span class="pl-s1">response</span>[<span class="pl-s">"input_ids"</span>] <span class="pl-c1">+</span> [<span class="pl-s1">tokenizer</span>.<span class="pl-c1">pad_token_id</span>]
    <span class="pl-k">if</span> <span class="pl-en">len</span>(<span class="pl-s1">input_ids</span>) <span class="pl-c1">&gt;</span> <span class="pl-c1">MAX_LENGTH</span>:
        <span class="pl-s1">input_ids</span> <span class="pl-c1">=</span> <span class="pl-s1">input_ids</span>[:<span class="pl-c1">MAX_LENGTH</span>]
        <span class="pl-s1">attention_mask</span> <span class="pl-c1">=</span> <span class="pl-s1">attention_mask</span>[:<span class="pl-c1">MAX_LENGTH</span>]
        <span class="pl-s1">labels</span> <span class="pl-c1">=</span> <span class="pl-s1">labels</span>[:<span class="pl-c1">MAX_LENGTH</span>]
    <span class="pl-k">return</span> {<span class="pl-s">"input_ids"</span>: <span class="pl-s1">input_ids</span>, <span class="pl-s">"attention_mask"</span>: <span class="pl-s1">attention_mask</span>, <span class="pl-s">"labels"</span>: <span class="pl-s1">labels</span>}

<span class="pl-c"># 推理函数</span>
<span class="pl-k">def</span> <span class="pl-en">predict</span>(<span class="pl-s1">messages</span>, <span class="pl-s1">model</span>, <span class="pl-s1">tokenizer</span>):
    <span class="pl-s1">device</span> <span class="pl-c1">=</span> <span class="pl-s">"cuda"</span>
    <span class="pl-s1">text</span> <span class="pl-c1">=</span> <span class="pl-s1">tokenizer</span>.<span class="pl-c1">apply_chat_template</span>(
        <span class="pl-s1">messages</span>,
        <span class="pl-s1">tokenize</span><span class="pl-c1">=</span><span class="pl-c1">False</span>,
        <span class="pl-s1">add_generation_prompt</span><span class="pl-c1">=</span><span class="pl-c1">True</span>
    )
    <span class="pl-s1">model_inputs</span> <span class="pl-c1">=</span> <span class="pl-en">tokenizer</span>([<span class="pl-s1">text</span>], <span class="pl-s1">return_tensors</span><span class="pl-c1">=</span><span class="pl-s">"pt"</span>).<span class="pl-c1">to</span>(<span class="pl-s1">device</span>)
    <span class="pl-s1">generated_ids</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-c1">generate</span>(
        <span class="pl-s1">model_inputs</span>.<span class="pl-c1">input_ids</span>,
        <span class="pl-s1">max_new_tokens</span><span class="pl-c1">=</span><span class="pl-c1">MAX_LENGTH</span>,
    )
    <span class="pl-s1">generated_ids</span> <span class="pl-c1">=</span> [
        <span class="pl-s1">output_ids</span>[<span class="pl-en">len</span>(<span class="pl-s1">input_ids</span>):] <span class="pl-k">for</span> <span class="pl-s1">input_ids</span>, <span class="pl-s1">output_ids</span> <span class="pl-c1">in</span> <span class="pl-en">zip</span>(<span class="pl-s1">model_inputs</span>.<span class="pl-c1">input_ids</span>, <span class="pl-s1">generated_ids</span>)
    ]
    <span class="pl-s1">response</span> <span class="pl-c1">=</span> <span class="pl-s1">tokenizer</span>.<span class="pl-c1">batch_decode</span>(<span class="pl-s1">generated_ids</span>, <span class="pl-s1">skip_special_tokens</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)[<span class="pl-c1">0</span>]
    <span class="pl-k">return</span> <span class="pl-s1">response</span>

<span class="pl-c"># 模型路径</span>
<span class="pl-s1">model_dir</span> <span class="pl-c1">=</span> <span class="pl-s">"/tmp/workspace/model/.cache/huggingface/download/naive"</span>

<span class="pl-c"># 加载 tokenizer 和模型</span>
<span class="pl-s1">tokenizer</span> <span class="pl-c1">=</span> <span class="pl-v">AutoTokenizer</span>.<span class="pl-c1">from_pretrained</span>(<span class="pl-s1">model_dir</span>, <span class="pl-s1">use_fast</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-s1">trust_remote_code</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">AutoModelForCausalLM</span>.<span class="pl-c1">from_pretrained</span>(<span class="pl-s1">model_dir</span>, <span class="pl-s1">device_map</span><span class="pl-c1">=</span><span class="pl-s">"auto"</span>, <span class="pl-s1">torch_dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-c1">bfloat16</span>)
<span class="pl-s1">model</span>.<span class="pl-c1">enable_input_require_grads</span>()  <span class="pl-c"># 开启梯度检查点支持</span>

<span class="pl-c"># 数据集路径</span>
<span class="pl-s1">train_dataset_path</span> <span class="pl-c1">=</span> <span class="pl-s">"/tmp/workspace/RussianEnglishDialogue/Dataset/format/train.jsonl"</span>
<span class="pl-s1">test_dataset_path</span> <span class="pl-c1">=</span> <span class="pl-s">"/tmp/workspace/RussianEnglishDialogue/Dataset/format/val.jsonl"</span>
<span class="pl-s1">train_jsonl_new_path</span> <span class="pl-c1">=</span> <span class="pl-s">"/tmp/workspace/RussianEnglishDialogue/Dataset/format/train_format.jsonl"</span>
<span class="pl-s1">test_jsonl_new_path</span> <span class="pl-c1">=</span> <span class="pl-s">"/tmp/workspace/RussianEnglishDialogue/Dataset/format/val_format.jsonl"</span>

<span class="pl-c"># 转换数据集格式</span>
<span class="pl-k">if</span> <span class="pl-c1">not</span> <span class="pl-s1">os</span>.<span class="pl-c1">path</span>.<span class="pl-c1">exists</span>(<span class="pl-s1">train_jsonl_new_path</span>):
    <span class="pl-en">dataset_jsonl_transfer</span>(<span class="pl-s1">train_dataset_path</span>, <span class="pl-s1">train_jsonl_new_path</span>)
<span class="pl-k">if</span> <span class="pl-c1">not</span> <span class="pl-s1">os</span>.<span class="pl-c1">path</span>.<span class="pl-c1">exists</span>(<span class="pl-s1">test_jsonl_new_path</span>):
    <span class="pl-en">dataset_jsonl_transfer</span>(<span class="pl-s1">test_dataset_path</span>, <span class="pl-s1">test_jsonl_new_path</span>)

<span class="pl-c"># 加载并处理训练集</span>
<span class="pl-s1">train_df</span> <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">read_json</span>(<span class="pl-s1">train_jsonl_new_path</span>, <span class="pl-s1">lines</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-s1">train_ds</span> <span class="pl-c1">=</span> <span class="pl-v">Dataset</span>.<span class="pl-c1">from_pandas</span>(<span class="pl-s1">train_df</span>)
<span class="pl-s1">train_dataset</span> <span class="pl-c1">=</span> <span class="pl-s1">train_ds</span>.<span class="pl-c1">map</span>(<span class="pl-s1">process_func</span>, <span class="pl-s1">remove_columns</span><span class="pl-c1">=</span><span class="pl-s1">train_ds</span>.<span class="pl-c1">column_names</span>)

<span class="pl-c"># 加载并处理验证集</span>
<span class="pl-s1">eval_df</span> <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">read_json</span>(<span class="pl-s1">test_jsonl_new_path</span>, <span class="pl-s1">lines</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-s1">eval_ds</span> <span class="pl-c1">=</span> <span class="pl-v">Dataset</span>.<span class="pl-c1">from_pandas</span>(<span class="pl-s1">eval_df</span>)
<span class="pl-s1">eval_dataset</span> <span class="pl-c1">=</span> <span class="pl-s1">eval_ds</span>.<span class="pl-c1">map</span>(<span class="pl-s1">process_func</span>, <span class="pl-s1">remove_columns</span><span class="pl-c1">=</span><span class="pl-s1">eval_ds</span>.<span class="pl-c1">column_names</span>)

<span class="pl-c"># 设置训练参数</span>
<span class="pl-s1">args</span> <span class="pl-c1">=</span> <span class="pl-en">TrainingArguments</span>(
    <span class="pl-s1">output_dir</span><span class="pl-c1">=</span><span class="pl-s">"/root/autodl-tmp/output/Qwen3-0.5B"</span>,
    <span class="pl-s1">per_device_train_batch_size</span><span class="pl-c1">=</span><span class="pl-c1">1</span>,
    <span class="pl-s1">per_device_eval_batch_size</span><span class="pl-c1">=</span><span class="pl-c1">1</span>,
    <span class="pl-s1">gradient_accumulation_steps</span><span class="pl-c1">=</span><span class="pl-c1">4</span>,
    <span class="pl-s1">eval_strategy</span><span class="pl-c1">=</span><span class="pl-s">"steps"</span>,
    <span class="pl-s1">eval_steps</span><span class="pl-c1">=</span><span class="pl-c1">100</span>,
    <span class="pl-s1">logging_steps</span><span class="pl-c1">=</span><span class="pl-c1">10</span>,
    <span class="pl-s1">num_train_epochs</span><span class="pl-c1">=</span><span class="pl-c1">2</span>,
    <span class="pl-s1">save_steps</span><span class="pl-c1">=</span><span class="pl-c1">400</span>,
    <span class="pl-s1">learning_rate</span><span class="pl-c1">=</span><span class="pl-c1">1e-4</span>,
    <span class="pl-s1">save_on_each_node</span><span class="pl-c1">=</span><span class="pl-c1">True</span>,
    <span class="pl-s1">gradient_checkpointing</span><span class="pl-c1">=</span><span class="pl-c1">True</span>,
    <span class="pl-s1">report_to</span><span class="pl-c1">=</span><span class="pl-s">"swanlab"</span>,
    <span class="pl-s1">run_name</span><span class="pl-c1">=</span><span class="pl-s">"qwen3-0.5B"</span>,
)

<span class="pl-c"># 初始化 Trainer</span>
<span class="pl-s1">trainer</span> <span class="pl-c1">=</span> <span class="pl-en">Trainer</span>(
    <span class="pl-s1">model</span><span class="pl-c1">=</span><span class="pl-s1">model</span>,
    <span class="pl-s1">args</span><span class="pl-c1">=</span><span class="pl-s1">args</span>,
    <span class="pl-s1">train_dataset</span><span class="pl-c1">=</span><span class="pl-s1">train_dataset</span>,
    <span class="pl-s1">eval_dataset</span><span class="pl-c1">=</span><span class="pl-s1">eval_dataset</span>,
    <span class="pl-s1">data_collator</span><span class="pl-c1">=</span><span class="pl-en">DataCollatorForSeq2Seq</span>(<span class="pl-s1">tokenizer</span><span class="pl-c1">=</span><span class="pl-s1">tokenizer</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-c1">True</span>),
)

<span class="pl-c"># 开始训练</span>
<span class="pl-s1">trainer</span>.<span class="pl-c1">train</span>()

<span class="pl-c"># 测试模型输出</span>
<span class="pl-s1">test_df</span> <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">read_json</span>(<span class="pl-s1">test_jsonl_new_path</span>, <span class="pl-s1">lines</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)[:<span class="pl-c1">3</span>]
<span class="pl-s1">test_text_list</span> <span class="pl-c1">=</span> []
<span class="pl-k">for</span> <span class="pl-s1">index</span>, <span class="pl-s1">row</span> <span class="pl-c1">in</span> <span class="pl-s1">test_df</span>.<span class="pl-c1">iterrows</span>():
    <span class="pl-s1">instruction</span> <span class="pl-c1">=</span> <span class="pl-s1">row</span>[<span class="pl-s">'instruction'</span>]
    <span class="pl-s1">input_value</span> <span class="pl-c1">=</span> <span class="pl-s1">row</span>[<span class="pl-s">'input'</span>]
    <span class="pl-s1">messages</span> <span class="pl-c1">=</span> [
        {<span class="pl-s">"role"</span>: <span class="pl-s">"system"</span>, <span class="pl-s">"content"</span>: <span class="pl-s">f"<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">instruction</span><span class="pl-kos">}</span></span>"</span>},
        {<span class="pl-s">"role"</span>: <span class="pl-s">"user"</span>, <span class="pl-s">"content"</span>: <span class="pl-s">f"<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">input_value</span><span class="pl-kos">}</span></span>"</span>}
    ]
    <span class="pl-s1">response</span> <span class="pl-c1">=</span> <span class="pl-en">predict</span>(<span class="pl-s1">messages</span>, <span class="pl-s1">model</span>, <span class="pl-s1">tokenizer</span>)
    <span class="pl-s1">response_text</span> <span class="pl-c1">=</span> <span class="pl-s">f"""</span>
<span class="pl-s">    Question: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">input_value</span><span class="pl-kos">}</span></span></span>
<span class="pl-s"></span>
<span class="pl-s">    LLM:<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">response</span><span class="pl-kos">}</span></span></span>
<span class="pl-s">    """</span>
    <span class="pl-s1">test_text_list</span>.<span class="pl-c1">append</span>(<span class="pl-s1">swanlab</span>.<span class="pl-c1">Text</span>(<span class="pl-s1">response_text</span>))
    <span class="pl-en">print</span>(<span class="pl-s1">response_text</span>)

<span class="pl-c"># 记录测试结果并结束实验</span>
<span class="pl-s1">swanlab</span>.<span class="pl-c1">log</span>({<span class="pl-s">"Prediction"</span>: <span class="pl-s1">test_text_list</span>})
<span class="pl-s1">swanlab</span>.<span class="pl-c1">finish</span>()</pre></div>
<hr>
<h1>解释：</h1>
<hr>
<h2>1. 基本环境与依赖导入</h2>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">import</span> <span class="pl-s1">json</span>
<span class="pl-k">import</span> <span class="pl-s1">pandas</span> <span class="pl-k">as</span> <span class="pl-s1">pd</span>
<span class="pl-k">import</span> <span class="pl-s1">torch</span>
<span class="pl-k">from</span> <span class="pl-s1">datasets</span> <span class="pl-k">import</span> <span class="pl-v">Dataset</span>
<span class="pl-k">from</span> <span class="pl-s1">transformers</span> <span class="pl-k">import</span> <span class="pl-v">AutoTokenizer</span>, <span class="pl-v">AutoModelForCausalLM</span>, <span class="pl-v">TrainingArguments</span>, <span class="pl-v">Trainer</span>, <span class="pl-v">DataCollatorForSeq2Seq</span>
<span class="pl-k">import</span> <span class="pl-s1">os</span>
<span class="pl-k">import</span> <span class="pl-s1">swanlab</span></pre></div>
<ul>
<li>
<p><strong>json、pandas</strong>：用于读取和处理 JSONL 格式的数据集。</p>
</li>
<li>
<p><strong>torch</strong>：PyTorch 底层库，用于张量运算和模型训练。</p>
</li>
<li>
<p><strong>datasets.Dataset</strong>：来自 Hugging Face 的 <code class="notranslate">datasets</code> 库，用于将 pandas DataFrame 封装成能被 Trainer 处理的数据集对象。</p>
</li>
<li>
<p><strong>transformers</strong> 相关：</p>
<ul>
<li><code class="notranslate">AutoTokenizer</code>：加载与模型对应的 tokenizer。</li>
<li><code class="notranslate">AutoModelForCausalLM</code>：加载因果语言模型（Causal LM），用于对话/文本生成类任务。</li>
<li><code class="notranslate">TrainingArguments</code>、<code class="notranslate">Trainer</code>：Hugging Face 官方的训练框架，用于管理训练超参、训练循环、保存模型等。</li>
<li><code class="notranslate">DataCollatorForSeq2Seq</code>：对齐（padding）和构建 batch，适用于 Seq2Seq 或 CausalLM 之类的任务。</li>
</ul>
</li>
<li>
<p><strong>os</strong>：主要用来配置环境变量和检查文件路径。</p>
</li>
<li>
<p><strong>swanlab</strong>：看起来是上传训练过程指标和日志到 SwanLab 平台的 SDK。</p>
</li>
</ul>
<hr>
<h2>2. 设置 SwanLab 项目与全局配置</h2>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># 设置 SwanLab 项目名称</span>
<span class="pl-s1">os</span>.<span class="pl-c1">environ</span>[<span class="pl-s">"SWANLAB_PROJECT"</span>] <span class="pl-c1">=</span> <span class="pl-s">"qwen3-sft-dialog"</span></pre></div>
<ul>
<li>这一行将环境变量 <code class="notranslate">SWANLAB_PROJECT</code> 设为 <code class="notranslate">"qwen3-sft-dialog"</code>，表示后续所有通过 <code class="notranslate">swanlab.log()</code>、<code class="notranslate">swanlab.finish()</code> 上传的日志都归属于这个项目。</li>
</ul>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># 定义提示（PROMPT）和最大序列长度</span>
<span class="pl-c1">PROMPT</span> <span class="pl-c1">=</span> <span class="pl-s">"你是一个对话助手，你需要根据用户的问题，给出相应的回答。"</span>
<span class="pl-c1">MAX_LENGTH</span> <span class="pl-c1">=</span> <span class="pl-c1">2048</span>

<span class="pl-c"># 更新 SwanLab 配置</span>
<span class="pl-s1">swanlab</span>.<span class="pl-c1">config</span>.<span class="pl-c1">update</span>({
    <span class="pl-s">"model"</span>: <span class="pl-s">"Qwen/Qwen3-0.5B"</span>,
    <span class="pl-s">"prompt"</span>: <span class="pl-c1">PROMPT</span>,
    <span class="pl-s">"data_max_length"</span>: <span class="pl-c1">MAX_LENGTH</span>,
})</pre></div>
<ul>
<li><code class="notranslate">PROMPT</code> 里定义了一个“系统提示”（system prompt），即对话模型在训练和推理时的最初上下文。</li>
<li><code class="notranslate">MAX_LENGTH=2048</code>：指定输入+输出的最大 token 数目。</li>
<li><code class="notranslate">swanlab.config.update(...)</code>：将模型名、提示语和最大序列长度一并上传到 SwanLab，让后台记录这一配置。</li>
</ul>
<hr>
<h2>3. 数据集格式转换</h2>
<h3>3.1 原始数据假设</h3>
<ul>
<li>
<p>原始训练集和验证集都是 JSONL 格式，每行都包含如下字段（示例）：</p>
<div class="highlight highlight-source-json"><pre class="notranslate">{
  <span class="pl-ent">"question"</span>: <span class="pl-s"><span class="pl-pds">"</span>用户的问题文本<span class="pl-pds">"</span></span>,
  <span class="pl-ent">"answer"</span>: <span class="pl-s"><span class="pl-pds">"</span>对应的回答文本<span class="pl-pds">"</span></span>
}</pre></div>
</li>
<li>
<p>代码中给出了 <code class="notranslate">train_dataset_path</code> 和 <code class="notranslate">test_dataset_path</code>，例如：</p>
<pre class="notranslate"><code class="notranslate">train_dataset_path = "/tmp/workspace/RussianEnglishDialogue/Dataset/format/train.jsonl"
test_dataset_path  = "/tmp/workspace/RussianEnglishDialogue/Dataset/format/val.jsonl"
</code></pre>
</li>
</ul>
<h3>3.2 转换成带 “instruction/input/output” 的格式</h3>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">def</span> <span class="pl-en">dataset_jsonl_transfer</span>(<span class="pl-s1">origin_path</span>, <span class="pl-s1">new_path</span>):
    <span class="pl-s1">messages</span> <span class="pl-c1">=</span> []
    <span class="pl-k">with</span> <span class="pl-en">open</span>(<span class="pl-s1">origin_path</span>, <span class="pl-s">"r"</span>) <span class="pl-k">as</span> <span class="pl-s1">file</span>:
        <span class="pl-k">for</span> <span class="pl-s1">line</span> <span class="pl-c1">in</span> <span class="pl-s1">file</span>:
            <span class="pl-s1">data</span> <span class="pl-c1">=</span> <span class="pl-s1">json</span>.<span class="pl-c1">loads</span>(<span class="pl-s1">line</span>)
            <span class="pl-s1">message</span> <span class="pl-c1">=</span> {
                <span class="pl-s">"instruction"</span>: <span class="pl-c1">PROMPT</span>,
                <span class="pl-s">"input"</span>: <span class="pl-s1">data</span>[<span class="pl-s">"question"</span>],
                <span class="pl-s">"output"</span>: <span class="pl-s1">data</span>[<span class="pl-s">"answer"</span>],
            }
            <span class="pl-s1">messages</span>.<span class="pl-c1">append</span>(<span class="pl-s1">message</span>)
    <span class="pl-k">with</span> <span class="pl-en">open</span>(<span class="pl-s1">new_path</span>, <span class="pl-s">"w"</span>, <span class="pl-s1">encoding</span><span class="pl-c1">=</span><span class="pl-s">"utf-8"</span>) <span class="pl-k">as</span> <span class="pl-s1">file</span>:
        <span class="pl-k">for</span> <span class="pl-s1">message</span> <span class="pl-c1">in</span> <span class="pl-s1">messages</span>:
            <span class="pl-s1">file</span>.<span class="pl-c1">write</span>(<span class="pl-s1">json</span>.<span class="pl-c1">dumps</span>(<span class="pl-s1">message</span>, <span class="pl-s1">ensure_ascii</span><span class="pl-c1">=</span><span class="pl-c1">False</span>) <span class="pl-c1">+</span> <span class="pl-s">"<span class="pl-cce">\n</span>"</span>)</pre></div>
<ul>
<li>
<p>这段函数会把原始的 <code class="notranslate">question</code>、<code class="notranslate">answer</code> 字段提取出来，然后写成新的 JSONL，每行长这样：</p>
<div class="highlight highlight-source-json"><pre class="notranslate">{
  <span class="pl-ent">"instruction"</span>: <span class="pl-s"><span class="pl-pds">"</span>你是一个对话助手，你需要根据用户的问题，给出相应的回答。<span class="pl-pds">"</span></span>,
  <span class="pl-ent">"input"</span>: <span class="pl-s"><span class="pl-pds">"</span>&lt;原来的 question 文本&gt;<span class="pl-pds">"</span></span>,
  <span class="pl-ent">"output"</span>: <span class="pl-s"><span class="pl-pds">"</span>&lt;原来的 answer 文本&gt;<span class="pl-pds">"</span></span>
}</pre></div>
</li>
<li>
<p>生成之后的文件路径是：</p>
<ul>
<li><code class="notranslate">train_format.jsonl</code>（训练集处理后）</li>
<li><code class="notranslate">val_format.jsonl</code>（验证集处理后）</li>
</ul>
</li>
<li>
<p>接下来的代码检查这两个新文件是否已经存在，如果不存在就调用上面的函数去生成：</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-s1">train_jsonl_new_path</span> <span class="pl-c1">=</span> <span class="pl-s">"/tmp/workspace/RussianEnglishDialogue/Dataset/format/train_format.jsonl"</span>
<span class="pl-s1">test_jsonl_new_path</span>  <span class="pl-c1">=</span> <span class="pl-s">"/tmp/workspace/RussianEnglishDialogue/Dataset/format/val_format.jsonl"</span>

<span class="pl-k">if</span> <span class="pl-c1">not</span> <span class="pl-s1">os</span>.<span class="pl-c1">path</span>.<span class="pl-c1">exists</span>(<span class="pl-s1">train_jsonl_new_path</span>):
    <span class="pl-en">dataset_jsonl_transfer</span>(<span class="pl-s1">train_dataset_path</span>, <span class="pl-s1">train_jsonl_new_path</span>)
<span class="pl-k">if</span> <span class="pl-c1">not</span> <span class="pl-s1">os</span>.<span class="pl-c1">path</span>.<span class="pl-c1">exists</span>(<span class="pl-s1">test_jsonl_new_path</span>):
    <span class="pl-en">dataset_jsonl_transfer</span>(<span class="pl-s1">test_dataset_path</span>, <span class="pl-s1">test_jsonl_new_path</span>)</pre></div>
</li>
</ul>
<hr>
<h2>4. 数据预处理函数（Tokenize &amp; 构造 labels）</h2>
<p>在对话或 SFT（Supervised Fine-Tuning）场景下，需要手动拼接“提示”“用户输入”“模型输出”三部分，并生成 <code class="notranslate">input_ids, attention_mask, labels</code>。labels 的构造方式是让模型只惩罚（loss）属于“回答”部分，而不惩罚“提示+用户输入”那段。</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">def</span> <span class="pl-en">process_func</span>(<span class="pl-s1">example</span>):
    <span class="pl-s1">input_ids</span>, <span class="pl-s1">attention_mask</span>, <span class="pl-s1">labels</span> <span class="pl-c1">=</span> [], [], []
    <span class="pl-s1">instruction</span> <span class="pl-c1">=</span> <span class="pl-en">tokenizer</span>(
        <span class="pl-s">f"&lt;|im_start|&gt;system<span class="pl-cce">\n</span><span class="pl-s1"><span class="pl-kos">{</span><span class="pl-c1">PROMPT</span><span class="pl-kos">}</span></span>&lt;|im_end|&gt;<span class="pl-cce">\n</span>"</span>
        <span class="pl-s">f"&lt;|im_start|&gt;user<span class="pl-cce">\n</span><span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">example</span>[<span class="pl-s">'input'</span>]<span class="pl-kos">}</span></span>&lt;|im_end|&gt;<span class="pl-cce">\n</span>"</span>
        <span class="pl-s">f"&lt;|im_start|&gt;assistant<span class="pl-cce">\n</span>"</span>,
        <span class="pl-s1">add_special_tokens</span><span class="pl-c1">=</span><span class="pl-c1">False</span>,
    )
    <span class="pl-s1">response</span> <span class="pl-c1">=</span> <span class="pl-en">tokenizer</span>(<span class="pl-s">f"<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">example</span>[<span class="pl-s">'output'</span>]<span class="pl-kos">}</span></span>"</span>, <span class="pl-s1">add_special_tokens</span><span class="pl-c1">=</span><span class="pl-c1">False</span>)

    <span class="pl-c"># 把 instruction 和 response 的 ids 拼接起来，末尾多一个 pad_token_id，用于强制生成结束</span>
    <span class="pl-s1">input_ids</span> <span class="pl-c1">=</span> <span class="pl-s1">instruction</span>[<span class="pl-s">"input_ids"</span>] <span class="pl-c1">+</span> <span class="pl-s1">response</span>[<span class="pl-s">"input_ids"</span>] <span class="pl-c1">+</span> [<span class="pl-s1">tokenizer</span>.<span class="pl-c1">pad_token_id</span>]
    <span class="pl-s1">attention_mask</span> <span class="pl-c1">=</span> <span class="pl-s1">instruction</span>[<span class="pl-s">"attention_mask"</span>] <span class="pl-c1">+</span> <span class="pl-s1">response</span>[<span class="pl-s">"attention_mask"</span>] <span class="pl-c1">+</span> [<span class="pl-c1">1</span>]

    <span class="pl-c"># labels：前面 instruction 的部分都标成 -100（表示这个位置的 token 不计算 loss），</span>
    <span class="pl-c"># 后面才是真正要让模型去预测的回复 token，最后一位 pad_token_id 也参与计算（可以算作一个结束标记）。</span>
    <span class="pl-s1">labels</span> <span class="pl-c1">=</span> [<span class="pl-c1">-</span><span class="pl-c1">100</span>] <span class="pl-c1">*</span> <span class="pl-en">len</span>(<span class="pl-s1">instruction</span>[<span class="pl-s">"input_ids"</span>]) <span class="pl-c1">+</span> <span class="pl-s1">response</span>[<span class="pl-s">"input_ids"</span>] <span class="pl-c1">+</span> [<span class="pl-s1">tokenizer</span>.<span class="pl-c1">pad_token_id</span>]

    <span class="pl-c"># 如果长度超过了 MAX_LENGTH，就进行截断</span>
    <span class="pl-k">if</span> <span class="pl-en">len</span>(<span class="pl-s1">input_ids</span>) <span class="pl-c1">&gt;</span> <span class="pl-c1">MAX_LENGTH</span>:
        <span class="pl-s1">input_ids</span> <span class="pl-c1">=</span> <span class="pl-s1">input_ids</span>[:<span class="pl-c1">MAX_LENGTH</span>]
        <span class="pl-s1">attention_mask</span> <span class="pl-c1">=</span> <span class="pl-s1">attention_mask</span>[:<span class="pl-c1">MAX_LENGTH</span>]
        <span class="pl-s1">labels</span> <span class="pl-c1">=</span> <span class="pl-s1">labels</span>[:<span class="pl-c1">MAX_LENGTH</span>]
    <span class="pl-k">return</span> {<span class="pl-s">"input_ids"</span>: <span class="pl-s1">input_ids</span>, <span class="pl-s">"attention_mask"</span>: <span class="pl-s1">attention_mask</span>, <span class="pl-s">"labels"</span>: <span class="pl-s1">labels</span>}</pre></div>
<ul>
<li><code class="notranslate">&lt;|im_start|&gt;system</code>、<code class="notranslate">&lt;|im_end|&gt;</code> 等是 QQE（Qwen Prompt）里约定的特殊分隔符，用来标记对话角色。</li>
<li>整个 <code class="notranslate">input_ids</code> 里先包含 system+user，然后紧跟 response。</li>
<li>训练时，模型只有在 “response” 部分才会计算交叉熵损失，前面的 instruction/user 填成 <code class="notranslate">-100</code>，这样就不会对它们算 loss。</li>
<li>最后强制在序列末尾加一个 <code class="notranslate">pad_token_id</code>，用作生成结束的标志。</li>
</ul>
<hr>
<h2>5. 推理（Inference）函数</h2>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-k">def</span> <span class="pl-en">predict</span>(<span class="pl-s1">messages</span>, <span class="pl-s1">model</span>, <span class="pl-s1">tokenizer</span>):
    <span class="pl-s1">device</span> <span class="pl-c1">=</span> <span class="pl-s">"cuda"</span>
    <span class="pl-s1">text</span> <span class="pl-c1">=</span> <span class="pl-s1">tokenizer</span>.<span class="pl-c1">apply_chat_template</span>(
        <span class="pl-s1">messages</span>,
        <span class="pl-s1">tokenize</span><span class="pl-c1">=</span><span class="pl-c1">False</span>,
        <span class="pl-s1">add_generation_prompt</span><span class="pl-c1">=</span><span class="pl-c1">True</span>
    )
    <span class="pl-s1">model_inputs</span> <span class="pl-c1">=</span> <span class="pl-en">tokenizer</span>([<span class="pl-s1">text</span>], <span class="pl-s1">return_tensors</span><span class="pl-c1">=</span><span class="pl-s">"pt"</span>).<span class="pl-c1">to</span>(<span class="pl-s1">device</span>)
    <span class="pl-s1">generated_ids</span> <span class="pl-c1">=</span> <span class="pl-s1">model</span>.<span class="pl-c1">generate</span>(
        <span class="pl-s1">model_inputs</span>.<span class="pl-c1">input_ids</span>,
        <span class="pl-s1">max_new_tokens</span><span class="pl-c1">=</span><span class="pl-c1">MAX_LENGTH</span>,
    )
    <span class="pl-c"># 这里去掉输入部分，只保留模型“新生成的” token</span>
    <span class="pl-s1">generated_ids</span> <span class="pl-c1">=</span> [
        <span class="pl-s1">output_ids</span>[<span class="pl-en">len</span>(<span class="pl-s1">input_ids</span>):] <span class="pl-k">for</span> <span class="pl-s1">input_ids</span>, <span class="pl-s1">output_ids</span> <span class="pl-c1">in</span> <span class="pl-en">zip</span>(<span class="pl-s1">model_inputs</span>.<span class="pl-c1">input_ids</span>, <span class="pl-s1">generated_ids</span>)
    ]
    <span class="pl-s1">response</span> <span class="pl-c1">=</span> <span class="pl-s1">tokenizer</span>.<span class="pl-c1">batch_decode</span>(<span class="pl-s1">generated_ids</span>, <span class="pl-s1">skip_special_tokens</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)[<span class="pl-c1">0</span>]
    <span class="pl-k">return</span> <span class="pl-s1">response</span></pre></div>
<ul>
<li>
<p><code class="notranslate">messages</code> 的格式如：</p>
<div class="highlight highlight-source-python"><pre class="notranslate">[
  {<span class="pl-s">"role"</span>: <span class="pl-s">"system"</span>,    <span class="pl-s">"content"</span>: <span class="pl-c1">PROMPT</span>},
  {<span class="pl-s">"role"</span>: <span class="pl-s">"user"</span>,      <span class="pl-s">"content"</span>: <span class="pl-s">"&lt;用户输入&gt;"</span>},
  <span class="pl-c"># 这里函数内部会自动在末尾加上一个 &lt;|im_start|&gt;assistant&gt; 的生成提示</span>
]</pre></div>
</li>
<li>
<p>先用 <code class="notranslate">apply_chat_template</code> 拼成一个完整的对话字符串（带角色分隔）交给 tokenizer 编码。</p>
</li>
<li>
<p>调用 <code class="notranslate">.generate(...)</code> 开始生成，<code class="notranslate">max_new_tokens=MAX_LENGTH</code> 表示“最多再生成这么多 token”。</p>
</li>
<li>
<p>生成后把完整的 <code class="notranslate">[input_ids + generated_ids]</code> 切割，只保留“模型后来新生成的那段”去解码。</p>
</li>
</ul>
<hr>
<h2>6. 预训练模型加载：<code class="notranslate">model_dir</code></h2>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-s1">model_dir</span> <span class="pl-c1">=</span> <span class="pl-s">"/tmp/workspace/model/.cache/huggingface/download/naive"</span>

<span class="pl-c"># 加载 tokenizer 和模型</span>
<span class="pl-s1">tokenizer</span> <span class="pl-c1">=</span> <span class="pl-v">AutoTokenizer</span>.<span class="pl-c1">from_pretrained</span>(<span class="pl-s1">model_dir</span>, <span class="pl-s1">use_fast</span><span class="pl-c1">=</span><span class="pl-c1">False</span>, <span class="pl-s1">trust_remote_code</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-v">AutoModelForCausalLM</span>.<span class="pl-c1">from_pretrained</span>(<span class="pl-s1">model_dir</span>, <span class="pl-s1">device_map</span><span class="pl-c1">=</span><span class="pl-s">"auto"</span>, <span class="pl-s1">torch_dtype</span><span class="pl-c1">=</span><span class="pl-s1">torch</span>.<span class="pl-c1">bfloat16</span>)
<span class="pl-s1">model</span>.<span class="pl-c1">enable_input_require_grads</span>()  <span class="pl-c"># 开启梯度检查点支持</span></pre></div>
<ul>
<li><code class="notranslate">model_dir</code> 指向一个本地目录，里面应该已经缓存好了预训练的基础模型权重（这里是 “Qwen/Qwen3-0.5B”）。</li>
<li>先用 <code class="notranslate">AutoTokenizer.from_pretrained(model_dir)</code> 把对应的 tokenizer 加载进来。</li>
<li>再用 <code class="notranslate">AutoModelForCausalLM.from_pretrained(model_dir, device_map="auto", torch_dtype=torch.bfloat16)</code> 把模型加载到显卡上，并以 bfloat16 的方式存储参数，以便减少显存占用。</li>
<li><code class="notranslate">model.enable_input_require_grads()</code> 用来启用梯度检查点（gradient checkpointing），在训练大模型时可以节省显存，不过会稍微牺牲一部分计算效率。</li>
</ul>
<hr>
<h2>7. 构建 Dataset 对象</h2>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># --- 训练集 ---</span>
<span class="pl-s1">train_df</span> <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">read_json</span>(<span class="pl-s1">train_jsonl_new_path</span>, <span class="pl-s1">lines</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-s1">train_ds</span> <span class="pl-c1">=</span> <span class="pl-v">Dataset</span>.<span class="pl-c1">from_pandas</span>(<span class="pl-s1">train_df</span>)
<span class="pl-s1">train_dataset</span> <span class="pl-c1">=</span> <span class="pl-s1">train_ds</span>.<span class="pl-c1">map</span>(<span class="pl-s1">process_func</span>, <span class="pl-s1">remove_columns</span><span class="pl-c1">=</span><span class="pl-s1">train_ds</span>.<span class="pl-c1">column_names</span>)

<span class="pl-c"># --- 验证集 ---</span>
<span class="pl-s1">eval_df</span> <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">read_json</span>(<span class="pl-s1">test_jsonl_new_path</span>, <span class="pl-s1">lines</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-s1">eval_ds</span> <span class="pl-c1">=</span> <span class="pl-v">Dataset</span>.<span class="pl-c1">from_pandas</span>(<span class="pl-s1">eval_df</span>)
<span class="pl-s1">eval_dataset</span> <span class="pl-c1">=</span> <span class="pl-s1">eval_ds</span>.<span class="pl-c1">map</span>(<span class="pl-s1">process_func</span>, <span class="pl-s1">remove_columns</span><span class="pl-c1">=</span><span class="pl-s1">eval_ds</span>.<span class="pl-c1">column_names</span>)</pre></div>
<ul>
<li><code class="notranslate">pd.read_json(..., lines=True)</code>：把新格式的 JSONL 文件读成一个 pandas DataFrame，DataFrame 列名是 <code class="notranslate">["instruction","input","output"]</code>。</li>
<li><code class="notranslate">Dataset.from_pandas(...)</code>：Hugging Face 的 <code class="notranslate">Dataset</code> 定义，用它可以把 pandas DataFrame 转换成一个能被 <code class="notranslate">Trainer</code> 直接消费的 dataset 对象。</li>
<li>然后 <code class="notranslate">.map(process_func, remove_columns=...)</code>：对每个样本都调用前面定义的 <code class="notranslate">process_func</code>，生成 <code class="notranslate">input_ids, attention_mask, labels</code> 三个字段，并删除原先的 <code class="notranslate">instruction,input,output</code> 列。</li>
<li>最终，<code class="notranslate">train_dataset</code> 和 <code class="notranslate">eval_dataset</code> 都是已经做过 tokenizer 和 label 构造的形式，且字段名固定为 <code class="notranslate">input_ids</code>、<code class="notranslate">attention_mask</code>、<code class="notranslate">labels</code>。</li>
</ul>
<hr>
<h2>8. 设置训练参数（<code class="notranslate">TrainingArguments</code>）</h2>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-s1">args</span> <span class="pl-c1">=</span> <span class="pl-en">TrainingArguments</span>(
    <span class="pl-s1">output_dir</span><span class="pl-c1">=</span><span class="pl-s">"/root/autodl-tmp/output/Qwen3-0.5B"</span>,
    <span class="pl-s1">per_device_train_batch_size</span><span class="pl-c1">=</span><span class="pl-c1">1</span>,
    <span class="pl-s1">per_device_eval_batch_size</span><span class="pl-c1">=</span><span class="pl-c1">1</span>,
    <span class="pl-s1">gradient_accumulation_steps</span><span class="pl-c1">=</span><span class="pl-c1">4</span>,
    <span class="pl-s1">eval_strategy</span><span class="pl-c1">=</span><span class="pl-s">"steps"</span>,
    <span class="pl-s1">eval_steps</span><span class="pl-c1">=</span><span class="pl-c1">100</span>,
    <span class="pl-s1">logging_steps</span><span class="pl-c1">=</span><span class="pl-c1">10</span>,
    <span class="pl-s1">num_train_epochs</span><span class="pl-c1">=</span><span class="pl-c1">2</span>,
    <span class="pl-s1">save_steps</span><span class="pl-c1">=</span><span class="pl-c1">400</span>,
    <span class="pl-s1">learning_rate</span><span class="pl-c1">=</span><span class="pl-c1">1e-4</span>,
    <span class="pl-s1">save_on_each_node</span><span class="pl-c1">=</span><span class="pl-c1">True</span>,
    <span class="pl-s1">gradient_checkpointing</span><span class="pl-c1">=</span><span class="pl-c1">True</span>,
    <span class="pl-s1">report_to</span><span class="pl-c1">=</span><span class="pl-s">"swanlab"</span>,
    <span class="pl-s1">run_name</span><span class="pl-c1">=</span><span class="pl-s">"qwen3-0.5B"</span>,
)</pre></div>
<p>重点参数说明：</p>
<ul>
<li>
<p><code class="notranslate">output_dir="/root/autodl-tmp/output/Qwen3-0.5B"</code></p>
<ul>
<li><strong>这是训练好的模型、检查点（checkpoint）和配置等最终保存的路径。</strong></li>
<li>训练过程中每隔 <code class="notranslate">save_steps=400</code> 会在这个目录下生成一次检查点，命名为 <code class="notranslate">checkpoint-400/</code>、<code class="notranslate">checkpoint-800/</code> 等等，并且训练结束后 Trainer 会把最终模型权重（和 tokenizer 配置）写到 <code class="notranslate">output_dir</code> 下。</li>
</ul>
</li>
<li>
<p><code class="notranslate">per_device_train_batch_size=1, per_device_eval_batch_size=1</code></p>
<ul>
<li>每张 GPU 上的 batch size 都是 1。</li>
</ul>
</li>
<li>
<p><code class="notranslate">gradient_accumulation_steps=4</code></p>
<ul>
<li>由于 batch size 太小，通过梯度累积把“等价 batch size”放大 4 倍，相当于每 4 步算一次梯度并更新一次模型。</li>
</ul>
</li>
<li>
<p><code class="notranslate">eval_strategy="steps", eval_steps=100</code></p>
<ul>
<li>每训练 100 步，就跑一次 eval。</li>
</ul>
</li>
<li>
<p><code class="notranslate">logging_steps=10</code></p>
<ul>
<li>每 10 步输出一次日志（loss、learning rate 等）。</li>
</ul>
</li>
<li>
<p><code class="notranslate">num_train_epochs=2</code></p>
<ul>
<li>总共训练数据迭代 2 个 epoch。</li>
</ul>
</li>
<li>
<p><code class="notranslate">learning_rate=1e-4</code></p>
<ul>
<li>学习率。</li>
</ul>
</li>
<li>
<p><code class="notranslate">save_on_each_node=True</code></p>
<ul>
<li>如果使用分布式训练，每个节点都会保存模型副本。</li>
</ul>
</li>
<li>
<p><code class="notranslate">gradient_checkpointing=True</code> 与 <code class="notranslate">model.enable_input_require_grads()</code> 配合表示启用梯度检查点，节省显存。</p>
</li>
<li>
<p><code class="notranslate">report_to="swanlab", run_name="qwen3-0.5B"</code></p>
<ul>
<li>将日志上报到 SwanLab，并且给这次实验起个名字。</li>
</ul>
</li>
</ul>
<hr>
<h2>9. 初始化 Trainer 并开始训练</h2>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-s1">trainer</span> <span class="pl-c1">=</span> <span class="pl-en">Trainer</span>(
    <span class="pl-s1">model</span><span class="pl-c1">=</span><span class="pl-s1">model</span>,
    <span class="pl-s1">args</span><span class="pl-c1">=</span><span class="pl-s1">args</span>,
    <span class="pl-s1">train_dataset</span><span class="pl-c1">=</span><span class="pl-s1">train_dataset</span>,
    <span class="pl-s1">eval_dataset</span><span class="pl-c1">=</span><span class="pl-s1">eval_dataset</span>,
    <span class="pl-s1">data_collator</span><span class="pl-c1">=</span><span class="pl-en">DataCollatorForSeq2Seq</span>(<span class="pl-s1">tokenizer</span><span class="pl-c1">=</span><span class="pl-s1">tokenizer</span>, <span class="pl-s1">padding</span><span class="pl-c1">=</span><span class="pl-c1">True</span>),
)

<span class="pl-c"># 开始训练</span>
<span class="pl-s1">trainer</span>.<span class="pl-c1">train</span>()</pre></div>
<ul>
<li><code class="notranslate">Trainer</code> 会根据上面传入的 <code class="notranslate">model</code>、<code class="notranslate">args</code>、<code class="notranslate">train_dataset</code>、<code class="notranslate">eval_dataset</code> 拆分训练循环、eval 循环，并且自动调用 <code class="notranslate">save_steps</code> 时保存检查点。</li>
<li><code class="notranslate">data_collator=DataCollatorForSeq2Seq(...)</code> 负责在一个 batch 中把不同长度的 <code class="notranslate">input_ids</code> 填充到相同长度、生成对应的 <code class="notranslate">attention_mask</code> 和 <code class="notranslate">labels</code>，以便 Hugging Face 可以直接把它送进模型做前向 / 反向传播。</li>
</ul>
<p><strong>训练过程</strong>：</p>
<ol>
<li>把 <code class="notranslate">train_dataset</code> 按照 <code class="notranslate">per_device_train_batch_size=1</code> 拆成一条条输入，4 步累积一次梯度。</li>
<li>每 100 步跑一次 <code class="notranslate">eval_dataset</code> 测试，并把结果打印出来。</li>
<li>每 400 步把当前 model state（包括模型权重、optimizer 状态、lr scheduler 状态等）保存到 <code class="notranslate">output_dir/checkpoint-400/</code>，以此类推。</li>
<li>2 个 epoch 结束后，<code class="notranslate">trainer.train()</code> 会把最终的模型（等同于 <code class="notranslate">model.save_pretrained(output_dir)</code>）自动写到 <code class="notranslate">output_dir</code>，覆盖之前的权重文件。</li>
</ol>
<hr>
<h2>10. 测试模型输出并将结果通过 SwanLab 上报</h2>
<p>训练完成后，我们用同样的验证集前 3 条数据做一次简单的推理，看看模型的回答与真实答案有何差距，并把推理结果也上传到 SwanLab。</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-c"># 读取验证集前三条</span>
<span class="pl-s1">test_df</span> <span class="pl-c1">=</span> <span class="pl-s1">pd</span>.<span class="pl-c1">read_json</span>(<span class="pl-s1">test_jsonl_new_path</span>, <span class="pl-s1">lines</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)[:<span class="pl-c1">3</span>]
<span class="pl-s1">test_text_list</span> <span class="pl-c1">=</span> []

<span class="pl-k">for</span> <span class="pl-s1">index</span>, <span class="pl-s1">row</span> <span class="pl-c1">in</span> <span class="pl-s1">test_df</span>.<span class="pl-c1">iterrows</span>():
    <span class="pl-s1">instruction</span> <span class="pl-c1">=</span> <span class="pl-s1">row</span>[<span class="pl-s">'instruction'</span>]
    <span class="pl-s1">input_value</span> <span class="pl-c1">=</span> <span class="pl-s1">row</span>[<span class="pl-s">'input'</span>]
    <span class="pl-s1">messages</span> <span class="pl-c1">=</span> [
        {<span class="pl-s">"role"</span>: <span class="pl-s">"system"</span>,    <span class="pl-s">"content"</span>: <span class="pl-s">f"<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">instruction</span><span class="pl-kos">}</span></span>"</span>},
        {<span class="pl-s">"role"</span>: <span class="pl-s">"user"</span>,      <span class="pl-s">"content"</span>: <span class="pl-s">f"<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">input_value</span><span class="pl-kos">}</span></span>"</span>}
    ]
    <span class="pl-s1">response</span> <span class="pl-c1">=</span> <span class="pl-en">predict</span>(<span class="pl-s1">messages</span>, <span class="pl-s1">model</span>, <span class="pl-s1">tokenizer</span>)
    <span class="pl-s1">response_text</span> <span class="pl-c1">=</span> <span class="pl-s">f"""</span>
<span class="pl-s">    Question: <span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">input_value</span><span class="pl-kos">}</span></span></span>
<span class="pl-s"></span>
<span class="pl-s">    LLM:<span class="pl-s1"><span class="pl-kos">{</span><span class="pl-s1">response</span><span class="pl-kos">}</span></span></span>
<span class="pl-s">    """</span>
    <span class="pl-s1">test_text_list</span>.<span class="pl-c1">append</span>(<span class="pl-s1">swanlab</span>.<span class="pl-c1">Text</span>(<span class="pl-s1">response_text</span>))
    <span class="pl-en">print</span>(<span class="pl-s1">response_text</span>)</pre></div>
<ul>
<li>
<p><code class="notranslate">predict(...)</code> 函数会把 <code class="notranslate">instruction</code> + <code class="notranslate">user input</code> 拼进去，调用 <code class="notranslate">model.generate</code> 生成回答。</p>
</li>
<li>
<p>把每条测试结果包装成 <code class="notranslate">swanlab.Text(...)</code>（SwanLab 定义的文本对象），放到列表里。</p>
</li>
<li>
<p>训练结束后，调用：</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-s1">swanlab</span>.<span class="pl-c1">log</span>({<span class="pl-s">"Prediction"</span>: <span class="pl-s1">test_text_list</span>})
<span class="pl-s1">swanlab</span>.<span class="pl-c1">finish</span>()</pre></div>
<ul>
<li>这意味着把测试结果一起上传到 SwanLab 平台，最后调用 <code class="notranslate">finish()</code> 报告实验结束。</li>
</ul>
</li>
</ul>
<hr>
<h2>11. “训练完的模型放在哪里了？”</h2>
<ul>
<li>
<p><strong>关键在 <code class="notranslate">TrainingArguments</code> 里的 <code class="notranslate">output_dir</code></strong>，代码中指定为：</p>
<div class="highlight highlight-source-python"><pre class="notranslate"><span class="pl-s1">output_dir</span><span class="pl-c1">=</span><span class="pl-s">"/root/autodl-tmp/output/Qwen3-0.5B"</span></pre></div>
</li>
<li>
<p>在训练过程中，<code class="notranslate">Trainer</code> 会在该目录下自动保存多个 checkpoint，例如：</p>
<pre class="notranslate"><code class="notranslate">/root/autodl-tmp/output/Qwen3-0.5B/checkpoint-400/
/root/autodl-tmp/output/Qwen3-0.5B/checkpoint-800/
...
</code></pre>
</li>
<li>
<p><strong>训练结束后</strong>，<code class="notranslate">Trainer.train()</code> 默认还会把当前的最优（或者最后一步）模型 <code class="notranslate">save_pretrained</code> 到 <code class="notranslate">output_dir</code>，并且将 tokenizer 相关配置也存入同一目录。最终目录结构可能类似如下：</p>
<pre class="notranslate"><code class="notranslate">/root/autodl-tmp/output/Qwen3-0.5B/
├── config.json                 # 模型配置文件
├── pytorch_model.bin           # (或) pytorch_model.safetensors，模型最终权重
├── tokenizer_config.json       # tokenizer 配置
├── special_tokens_map.json     # 如果用了特殊 token
├── vocab.json / merges.txt     # 根据对应 tokenizer 类型
├── training_args.bin           # 保存了训练参数的二进制文件
├── checkpoint-400/             # 若中途保存了 checkpoint，会看到这样若干子文件夹
│   ├── config.json
│   ├── ...（各 checkpoint 权重、optimizer 状态）
├── checkpoint-800/
│   └── ...
└── last_checkpoint/  (或类似的)
    └── ...
</code></pre>
</li>
<li>
<p><strong>也就是说</strong>，你只要进到 <code class="notranslate">/root/autodl-tmp/output/Qwen3-0.5B</code> 这个文件夹，就能找到：</p>
<ol>
<li>每次 <code class="notranslate">save_steps</code> 保存的中间 checkpoint</li>
<li>训练结束后最终写入的模型权重和 tokenizer 等文件</li>
</ol>
</li>
</ul>
<hr>
<h2>12. 小结</h2>
<ol>
<li>
<p><strong>代码流程</strong>：</p>
<ul>
<li>设置环境（SwanLab、prompt、最大长度）</li>
<li>把原始的 QA 数据转换成带 <code class="notranslate">instruction</code> / <code class="notranslate">input</code> / <code class="notranslate">output</code> 格式的 JSONL</li>
<li>定义 <code class="notranslate">process_func</code>，拼接 system/user/assistant 三段，生成 <code class="notranslate">input_ids, attention_mask, labels</code></li>
<li>加载预训练模型和 tokenizer</li>
<li>用 <code class="notranslate">Dataset.from_pandas</code> + <code class="notranslate">.map(process_func)</code> 得到能用于 Hugging Face Trainer 的格式化数据集</li>
<li>构造 <code class="notranslate">TrainingArguments</code>，指定 <code class="notranslate">output_dir</code>、训练超参、将日志上报给 SwanLab</li>
<li>初始化 <code class="notranslate">Trainer</code> 并执行 <code class="notranslate">.train()</code>，训练完成后会把模型保存到 <code class="notranslate">output_dir</code></li>
<li>最后用前三条验证集数据跑一次 <code class="notranslate">.generate()</code>，把预测结果上传到 SwanLab</li>
</ul>
</li>
<li>
<p><strong>训练结束后模型存放的位置</strong>：</p>
<ul>
<li>
<p>最终模型（以及所有中间 checkpoint）都存放在 <code class="notranslate">TrainingArguments</code> 中 <code class="notranslate">output_dir</code> 指定的目录下。</p>
</li>
<li>
<p>本例中是：</p>
<pre class="notranslate"><code class="notranslate">/root/autodl-tmp/output/Qwen3-0.5B
</code></pre>
</li>
<li>
<p>进入该文件夹后，你会看到 <code class="notranslate">config.json, pytorch_model.bin, tokenizer_config.json, …</code> 等一系列文件，以及若干 <code class="notranslate">checkpoint-XXX</code> 子文件夹。</p>
</li>
</ul>
</li>
</ol>
<p>只要在训练完毕后，通过文件系统浏览或脚本 <code class="notranslate">ls /root/autodl-tmp/output/Qwen3-0.5B</code>，就能确认模型确实保存在哪个子目录下。</p></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://jibinghu.github.io">ZOMBIE_</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if("05/28/2024"!=""){
    var startSite=new Date("05/28/2024");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z', 'copy': 'M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z', 'check': 'M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","jibinghu/jibinghu.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}

document.addEventListener('DOMContentLoaded', () => {
    const createClipboardHTML = (codeContent, additionalClasses = '') => `
        <pre class="notranslate"><code class="notranslate">${codeContent}</code></pre>
        <div class="clipboard-container position-absolute right-0 top-0 ${additionalClasses}">
            <clipboard-copy class="ClipboardButton btn m-2 p-0" role="button" style="display: inherit;">
                <svg height="16" width="16" class="octicon octicon-copy m-2"><path d="${IconList["copy"]}"></path></svg>
                <svg height="16" width="16" class="octicon octicon-check color-fg-success m-2 d-none"><path d="${IconList["check"]}"></path></svg>
            </clipboard-copy>
            <div class="copy-feedback">Copied!</div>
        </div>
    `;

    const handleCodeElements = (selector = '') => {
        document.querySelectorAll(selector).forEach(codeElement => {
            const codeContent = codeElement.innerHTML;
            const newStructure = document.createElement('div');
            newStructure.className = 'snippet-clipboard-content position-relative overflow-auto';
            newStructure.innerHTML = createClipboardHTML(codeContent);

            const parentElement = codeElement.parentElement;
            if (selector.includes('highlight')) {
                parentElement.insertBefore(newStructure, codeElement.nextSibling);
                parentElement.removeChild(codeElement);
            } else {
                parentElement.parentElement.replaceChild(newStructure, parentElement);
            }
        });
    };

    handleCodeElements('pre.notranslate > code.notranslate');
    handleCodeElements('div.highlight > pre.notranslate');

    let currentFeedback = null;
    document.querySelectorAll('clipboard-copy').forEach(copyButton => {
        copyButton.addEventListener('click', () => {
            const codeContent = copyButton.closest('.snippet-clipboard-content').innerText;
            const tempTextArea = document.createElement('textarea');
            tempTextArea.value = codeContent;
            document.body.appendChild(tempTextArea);
            tempTextArea.select();
            document.execCommand('copy');
            document.body.removeChild(tempTextArea);

            const copyIcon = copyButton.querySelector('.octicon-copy');
            const checkIcon = copyButton.querySelector('.octicon-check');
            const copyFeedback = copyButton.nextElementSibling;

            if (currentFeedback && currentFeedback !== copyFeedback) {currentFeedback.style.display = 'none';}
            currentFeedback = copyFeedback;

            copyIcon.classList.add('d-none');
            checkIcon.classList.remove('d-none');
            copyFeedback.style.display = 'block';
            copyButton.style.borderColor = 'var(--color-success-fg)';

            setTimeout(() => {
                copyIcon.classList.remove('d-none');
                checkIcon.classList.add('d-none');
                copyFeedback.style.display = 'none';
                copyButton.style.borderColor = '';
            }, 2000);
        });
    });
});

</script>


</html>
